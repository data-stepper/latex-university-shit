\documentclass[10pt]{article}
 
\usepackage[margin=1in]{geometry} 
\usepackage{amsmath,amsthm,amssymb, graphicx, multicol, array}
 
\newcommand{\R}{\mathbb{R}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\A}{\mathcal{A}}
\newcommand{\D}{\mathcal{D}}
\newcommand{\B}{\mathbb{B}}
\newcommand{\normal}{\mathcal{N}}
\newcommand{\gap}{\,\vert\,}
\newcommand{\beh}{\textit{Behauptung. }}

\setlength{\parindent}{0pt}
 
\newenvironment{Aufgabe}[2][Aufgabe]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newenvironment{amatrix}[1]{%
  \left(\begin{array}{@{}*{#1}{c}|c@{}}
}{%
  \end{array}\right)
}

\begin{document}
 
\title{ \textbf{Maßtheoretische Konzepte der Stochastik \\ -- Übungsblatt \#08 --} }

\author{Amir Miri Lavasani (7310114), Bent Müller (7302332),
        Michael Hermann (6981007)}
\maketitle

\begin{Aufgabe}{1} % #1
\end{Aufgabe}

Seien 
\begin{align*}
	V \quad&\hat{=}\quad \text{"Körperlänge vom Vater"} \\
	S \quad&\hat{=}\quad \text{"Körperlänge vom Sohn"} \\
	S_{V=x} \quad&\hat{=}\quad \text{"Körperlänge des Sohnes, gegeben dass V=x"}
\end{align*} 
Aus der Aufgabenstellung ist gegeben:
\begin{align*}
	V &\sim \normal(a,\sigma^2) \\
	S_{V=x} &\sim \normal(x+b(a-x),\tau^2)
\end{align*}

Sei $P^V$ die Verteilung von $V$. Dann bezeichne $\varphi_V$ die $\lambda$-Dichte von $P^V$, wobei 
$\lambda$ das Lebesgue-Maß bezeichnet. Analog sei $P^{S_{V=x}}$ die Verteilung von $S_{V=x}$ mit 
$\lambda$-Dichte $\varphi_{S_{V=x}}(x, \cdot) = K(x, \cdot)$. Dies wählen wir als unseren Markov-Kern.
Wohlgemerkt sind die Verteilungen $\sigma$-endlich weil diese Wahrscheinlichkeitsmaße sind.
Nach Blatt 6 Aufgabe 2 hat die gemeinsame Verteilung von 
$V$ und $S$, $P^V \otimes P^{S_{V=x}} =: P^{(V,S)}$, die $\lambda^2$-Dichte $\varphi_V \cdot 
\varphi_{S_{V=x}}(x, \cdot) =: \varphi_{(V,S)}$. \\

% Hier geht der mittlere Teil los

Nun wollen wir zeigen, dass die gemeinsame Verteilung von $P ^{V, S}$ eine 2-dimensionale Normalverteilung ist.

Hierfür berechnen wir das Produkt der beiden Dichten:
\begin{align*}
	\varphi_{\left(
		V, S
\right) } (x, y) &= 
\frac{ 1 }{ \sqrt{2 \pi} \sigma } e ^{\frac{ -1 }{ 2 } \left(
		\frac{ x-a }{ \sigma }
\right) ^2 } \cdot
\frac{ 1 }{ \sqrt{2 \pi} \tau } e ^{\frac{ -1 }{ 2 } \left(
		\frac{ y - (x + b(a-x)) }{ \tau }
\right) ^2 }
\end{align*}

Um nun die Kovarianzmatrix zu berechnen müssen wir den Exponenten ausmultiplizieren.
Bei der exakten Nachrechnung dieser sind wir leider gescheitert.
\\

Der bedingte Erwartungswert $E[S|V]$ der Körperlänge S der Söhne gegeben der Körperlänge V der Väter ist 
$E[S|V] = x+b(a-x)$. Nach Satz $5.9$ (i) ist der Erwartungswert der Körperlänge der Söhne gegeben durch
\begin{align*}
	E[S] &= E[E[S|V]] 	   \\ 
		&= E[V + b(a-V)]   \\ 
		&= E[(1-b)V + ba]  \\ 
		&= (1-b)E[V] + ba  \\
		&= (1-b)a + ba 	   \\
		&= a.
\end{align*}   

Daraus folgt für den Mittelwertvektor $\mu$, dass $\mu = \begin{pmatrix} a \\ a\end{pmatrix}$. Um zu zeigen, dass
$P^{V} = P^{S}$ bleibt noch zu zeigen, dass die Varianzen von $V$ und $S$ übereinstimmen. Aus der Kovarianzmatrix 
lesen wir heraus, dass $Var(S) = \tau^2 + (b-1)^2\sigma^2$. Aus der Aufgabenstellung ist bekannt, dass $\tau^2 = 37,5$ 
und $Var(V) = \sigma^2 = 50$. Also ist $\tau^2 = 0,75\sigma^2$. Aus $b=0,5$ folgt schließlich $Var(S) = \sigma^2 = 
Var(V)$. Die Körperlänge der Söhne ist also genauso verteilt wie die der Väter.  

\begin{Aufgabe}{2} % #2
	Es seien $(\Omega, \mathcal{A})$ und $(S, \mathcal{D})$ Maßräume und $Y:(\Omega, \mathcal{A}) \rightarrow (S, \mathcal{D})$ und $Z:(\Omega, \sigma(Y)) \rightarrow (\mathbb{R}, \mathbb{B})$ jeweils messbare Abbildungen. 
\end{Aufgabe}

\beh Es existiert eine Abbildung $h:(S, \mathcal{D}) \rightarrow  (\mathbb{R}, \mathbb{B})$, so dass $Z = h \circ Y$.

\begin{proof}[Beweis]
	Es sei $Y$ eine beliebige, aber fest gewählte Abbildung von der gewünschten Form. Wir zeigen die Aussage für alle Abbildungen $Z:(\Omega, \sigma(Y)) \rightarrow (\mathbb{R}, \mathbb{B})$ mit Hilfe von algebraischer Induktion. \\

	Sei zuerst $Z$ eine Elementarfunktion, dh. $Z = \sum_{i=1}^n a_i \mathbbm{1}_{A_i}$ für paarweise disjunkten Mengen $A_i \in \sigma(Y)$ und $0 < a_i \in \bar{\mathbb{R}}$ für $1 \leq i \leq n$. 
	Da die Mengen $A_i$ alle in $\sigma(Y)$ liegen, ist jede Menge $A_i$ das Urbild $Y^{-1}(B_i)$ einer Menge $B_i \in \mathcal{D}$. Für $\omega \in \Omega$ gilt dann $\mathbbm{1}_{A_i}(\omega) = \mathbbm{1}_{B_i}(Y(\omega)) = \mathbbm{1}_{B_i} \circ Y (\omega)$
	für alle $1 \leq i \leq n$ und es folgt $Z = h \circ Y$ für $h \coloneqq \sum_{i=1}^n a_i \mathbbm{1}_{B_i}$. Die so definierte Funktion ist natürlich auch messbar. \\

	Ist $Z$ eine beliebige nicht-negative Funktion $(\Omega, \sigma(Y)) \rightarrow (\mathbb{R}, \mathbb{B})$, dann gibt es eine Folge von Elementarfunktionen $(Z_k)_{k \in \mathbb{N}}$  mit $Z_k \uparrow Z$. Wie wir soeben gesehen haben, gibt es zu jedem $Z_k$ eine 	messbare Funktion $h_k$ mit $Z_k = h_k \circ Y$ und damit $h_k \circ Y \uparrow Z$ für $k \to \infty$. Aus der Monotonie der Konvergenz folgt die Existenz und Messbarkeit einer Funktion $h \coloneqq \lim_{k \to \infty} h_k$ und für diese gilt $Z = h \circ Y$. \\

	Ist schliesslich $Z$ eine beliebige messbare Funktion $(\Omega, \sigma(Y)) \rightarrow (\mathbb{R}, \mathbb{B})$, dann zerlegen wir sie in Positiv- und Negativteil, dh. $Z = Z^+ - Z^-$, und erhalten aus dem vorigen Schritt zwei Funktionen $h^+$ und $h^-$ mit den
	gewünschten Eigenschaften. Für $h \coloneqq h^+ - h^-$ gilt dann $h \circ Y = h^+\circ Y - h^-\circ Y = Z^+ - Z^- = Z$.  
\end{proof}

\begin{Aufgabe}{3} % #3
	Seien $X : (\Omega, \A, P)\to (\R, \B)$ und $Z_i : (\Omega, \A)\to (S_i, \D_i), i = 1,2$. Seien außerdem $X$ und $Z_2$
	bedingt stochastisch unabhängig gegeben $Z_1$.
\end{Aufgabe}

\beh Für alle Funktionen $h: (\R,\B)\to (\R,\B)$ mit $E[\,\vert h(X) \vert\,] < \infty$ und alle Mengen $D_2\in\D$ gilt 
	\begin{align*}
		E[h(X)1_{D_2}(Z_2) \,\vert\, Z_1] = E[h(X) \,\vert\, Z_1] \cdot E[1_{D_2}(Z_2) \gap Z_1] \quad\text{$P-$f.s.}
	\end{align*}

\begin{proof}[Beweis]
	Sei $B\in\A$. Da $X$ und $Z_2$ bedingt unabhängig gegeben $Z_1$ sind, sind es auch $1_B(X)$ und $1_{D_2}(Z_2)$. Also gilt 
	\begin{align*}
		E[1_B(X)1_{D_2}(Z_2) \gap Z_1] = E[1_B(X) \gap Z_1]\cdot E[1_{D_2}(Z_2) \gap Z_1].
	\end{align*}

	Sei nun $X_n := \sum_{i=1}^{n} c_i\cdot 1_{B_i}$ für $B_i\in\A,\, c_i\in\R_{>0},\, n\in\N$. Dann ist $X_n$ eine einfache Funktion. 
	Aus der Linearität des bedingten Erwartungswertes, folgt:
	\begin{align*}
		E[X_n(X)1_{D_2}(Z_2) \gap Z_1] &= \sum_{i=1}^{n} c_i E[1_{B_i}(X) 1_{D_2}(Z_2) \gap Z_1]		\\
									   &= \sum_{i=1}^{n} c_i E[1_{B_i}(X) \gap Z_1] \cdot E[1_{D_2}(Z_2) \gap Z_1] 
									   = E[X_n(X) \gap Z_1] \cdot E[1_{D_2}(Z_2) \gap Z_1].
	\end{align*}

	Die Behauptung gilt also für einfache Funktionen. Sei nun $h \geq 0$. Wähle $X_n$, so dass $X_n \uparrow h$ $P-$f.s. 
	Daraus folgt, dass auch $X_n 1_{D_2} \uparrow h 1_{D_2}$ $P-$f.s. Mit $5.3$ (vii) gilt dann ($P-$f.s.)
	\begin{align*}
		E[h(X) 1_{D_2}(Z_2) \gap Z_1] &= \lim_{n\to\infty} E[X_n(X) 1_{D_2}(Z_2) \gap Z_1]  \\
									  &= \lim_{n\to\infty} E[X_n(X) \gap Z_1]\cdot E[1_{D_2}(Z_2) \gap Z_1]  
									  = E[h(X) \gap Z_1]\cdot E[1_{D_2}(Z_2) \gap Z_1]. 
	\end{align*}

	Schließlich sei $h$ eine beliebige Funktion. Setze $h = h^+ - h^-$ mit $h^+, h^- \geq 0$. Dann folgt zusammen mit 
	der Linearität des bedingten Erwartungswertes und den vorherigen Ergebnissen ($P-$f.s.)
	\begin{align*}
		E[h(X) 1_{D_2}(Z_2) \gap Z_1] &= E[h^+(X) 1_{D_2}(Z_2) \gap Z_1] - E[h(X)^- 1_{D_2}(Z_2) \gap Z_1] 								    \\
									  &= E[h^+(X) \gap Z_1]\cdot E[1_{D_2}(Z_2) \gap Z_1] - E[h^-(X) \gap Z_1]\cdot E[1_{D_2}(Z_2) \gap Z_1] \\
									  &= E[h(X) \gap Z_1]\cdot E[1_{D_2}(Z_2) \gap Z_1].
	\end{align*}
\end{proof}

\end{document}

