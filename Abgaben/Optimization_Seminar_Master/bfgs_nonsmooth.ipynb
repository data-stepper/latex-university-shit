{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-01-07T12:22:40.831725Z",
     "start_time": "2025-01-07T12:22:40.737502Z"
    }
   },
   "source": [
    "from itertools import product\n",
    "\n",
    "from math import inf\n",
    "import numpy as np\n",
    "import scipy.special\n",
    "from sklearn.datasets import make_regression\n",
    "\n",
    "\n",
    "def bfgs(f, f_grad, x0, max_iter=500, tol=1e-6):\n",
    "    \"\"\"\n",
    "    BFGS algorithm for unconstrained optimization.\n",
    "\n",
    "    Args:\n",
    "        f: The objective function to be minimized.\n",
    "        f_grad: The gradient of the objective function.\n",
    "        x0: The initial guess for the solution.\n",
    "        max_iter: The maximum number of iterations.\n",
    "        tol: The tolerance for convergence.\n",
    "\n",
    "    Returns:\n",
    "        A tuple containing:\n",
    "            - The optimal solution found.\n",
    "            - The value of the objective function at the solution.\n",
    "            - The number of iterations performed.\n",
    "    \"\"\"\n",
    "\n",
    "    x = x0\n",
    "    n = len(x0)\n",
    "    B = np.eye(n)  # Initial approximation of the Hessian inverse\n",
    "    grad = f_grad(x)\n",
    "\n",
    "    for i in range(max_iter):\n",
    "        # Compute search direction\n",
    "        p = -np.linalg.solve(B, grad)\n",
    "\n",
    "        # Perform line search\n",
    "        t = line_search(f, f_grad, x, p)\n",
    "\n",
    "        # Update x\n",
    "        x_next = x + t * p\n",
    "\n",
    "        # Update B (BFGS update)\n",
    "        s = x_next - x\n",
    "        y = f_grad(x_next) - grad\n",
    "        if np.dot(s, y) > 0:  # Ensure positive definiteness\n",
    "            rho = 1 / np.dot(s, y)\n",
    "            B = (np.eye(n) - rho * np.outer(s, y)) @ B @ (np.eye(n) - rho * np.outer(y, s)) + rho * np.outer(s, s)\n",
    "        else:\n",
    "            B = np.eye(n)\n",
    "\n",
    "        # Update x and gradient\n",
    "        x = x_next\n",
    "        grad = f_grad(x)\n",
    "\n",
    "        # Check for convergence\n",
    "        if np.linalg.norm(grad) < tol:\n",
    "            break\n",
    "\n",
    "    return x, f(x), i + 1\n",
    "\n",
    "\n",
    "def line_search(f, f_grad, x, p, c1=1e-4, c2=0.9):\n",
    "    \"\"\"\n",
    "    Backtracking line search.\n",
    "\n",
    "    Args:\n",
    "        f: The objective function.\n",
    "        f_grad: The gradient of the objective function.\n",
    "        x: The current point.\n",
    "        p: The search direction.\n",
    "        c1: The Armijo condition parameter.\n",
    "        c2: The curvature condition parameter.\n",
    "\n",
    "    Returns:\n",
    "        The step size that satisfies the Wolfe conditions.\n",
    "    \"\"\"\n",
    "    assert 0 < c1 < c2 < 1\n",
    "    beta = inf\n",
    "    alpha = 0.0\n",
    "    t = 1.0\n",
    "    s = np.dot(f_grad(x), p)\n",
    "\n",
    "    for iter in range(10_000):\n",
    "        # Check Armijo Condition\n",
    "        if f(x + t * p) - f(x) <= c1 * t * s:\n",
    "            # Check Wolfe Condition\n",
    "            if f_grad(x + t * p) @ p >= c2 * s:\n",
    "                break\n",
    "            else:\n",
    "                # Wolfe failed\n",
    "                alpha = t\n",
    "        else:\n",
    "            # Armijo failed\n",
    "            beta = t\n",
    "        if beta < inf:\n",
    "            t = (alpha + beta) / 2\n",
    "        else:\n",
    "            t = 2 * alpha\n",
    "\n",
    "    if iter == 10_000 - 1:\n",
    "        raise ValueError(\"Line search did not converge at x={x}, p={p}, t={t:.2e}\".format(x=x, p=p, t=t))\n",
    "    return t\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "def f(x):\n",
    "    return x[0] ** 2 + 5 * x[1] ** 4 + 0.1 * abs(x[0])\n",
    "\n",
    "\n",
    "def f_grad(x, dx=1e-6):\n",
    "    n = len(x)\n",
    "    grad = np.zeros(n)\n",
    "    f_val = f(x)\n",
    "    for i in range(n):\n",
    "        x_plus = x.copy()\n",
    "        x_plus[i] += dx\n",
    "        grad[i] = (f(x_plus) - f_val) / dx\n",
    "    return grad\n",
    "\n",
    "\n",
    "x0 = np.array([3.0, 4.0])\n",
    "x_opt, f_opt, iterations = bfgs(f, f_grad, x0)\n",
    "\n",
    "print(\"Optimal solution:\", x_opt)\n",
    "print(\"Optimal value:\", f_opt)\n",
    "print(\"Iterations:\", iterations)"
   ],
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Line search did not converge at x=[ 4.55014146e-08 -1.22990350e-01], p=[-1.46540516e+04  4.90101639e-02], t=6.21e-12",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[207], line 118\u001B[0m\n\u001B[1;32m    115\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m grad\n\u001B[1;32m    117\u001B[0m x0 \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39marray([\u001B[38;5;241m3.0\u001B[39m, \u001B[38;5;241m4.0\u001B[39m])\n\u001B[0;32m--> 118\u001B[0m x_opt, f_opt, iterations \u001B[38;5;241m=\u001B[39m \u001B[43mbfgs\u001B[49m\u001B[43m(\u001B[49m\u001B[43mf\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mf_grad\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mx0\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    120\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mOptimal solution:\u001B[39m\u001B[38;5;124m\"\u001B[39m, x_opt)\n\u001B[1;32m    121\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mOptimal value:\u001B[39m\u001B[38;5;124m\"\u001B[39m, f_opt)\n",
      "Cell \u001B[0;32mIn[207], line 37\u001B[0m, in \u001B[0;36mbfgs\u001B[0;34m(f, f_grad, x0, max_iter, tol)\u001B[0m\n\u001B[1;32m     34\u001B[0m p \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m-\u001B[39mnp\u001B[38;5;241m.\u001B[39mlinalg\u001B[38;5;241m.\u001B[39msolve(B, grad)\n\u001B[1;32m     36\u001B[0m \u001B[38;5;66;03m# Perform line search\u001B[39;00m\n\u001B[0;32m---> 37\u001B[0m t \u001B[38;5;241m=\u001B[39m \u001B[43mline_search\u001B[49m\u001B[43m(\u001B[49m\u001B[43mf\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mf_grad\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mp\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     39\u001B[0m \u001B[38;5;66;03m# Update x\u001B[39;00m\n\u001B[1;32m     40\u001B[0m x_next \u001B[38;5;241m=\u001B[39m x \u001B[38;5;241m+\u001B[39m t \u001B[38;5;241m*\u001B[39m p\n",
      "Cell \u001B[0;32mIn[207], line 100\u001B[0m, in \u001B[0;36mline_search\u001B[0;34m(f, f_grad, x, p, c1, c2)\u001B[0m\n\u001B[1;32m     97\u001B[0m         t \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m2\u001B[39m \u001B[38;5;241m*\u001B[39m alpha\n\u001B[1;32m     99\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28miter\u001B[39m \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m10_000\u001B[39m \u001B[38;5;241m-\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[0;32m--> 100\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mLine search did not converge at x=\u001B[39m\u001B[38;5;132;01m{x}\u001B[39;00m\u001B[38;5;124m, p=\u001B[39m\u001B[38;5;132;01m{p}\u001B[39;00m\u001B[38;5;124m, t=\u001B[39m\u001B[38;5;132;01m{t:.2e}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(x\u001B[38;5;241m=\u001B[39mx, p\u001B[38;5;241m=\u001B[39mp, t\u001B[38;5;241m=\u001B[39mt))\n\u001B[1;32m    101\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m t\n",
      "\u001B[0;31mValueError\u001B[0m: Line search did not converge at x=[ 4.55014146e-08 -1.22990350e-01], p=[-1.46540516e+04  4.90101639e-02], t=6.21e-12"
     ]
    }
   ],
   "execution_count": 207
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-07T11:06:00.296930Z",
     "start_time": "2025-01-07T11:06:00.259546Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from scipy.optimize import minimize\n",
    "from sklearn.datasets import make_regression\n",
    "\n",
    "x, y = make_regression(n_samples=100, n_features=10, noise=10, n_informative=3, random_state=42)\n",
    "\n",
    "\n",
    "def f(beta):\n",
    "    # enet criterion\n",
    "    y_hat = x @ beta\n",
    "    mse = np.mean((y - y_hat) ** 2)\n",
    "    penalty = 10.0 * np.sum(np.abs(beta)) + 5 * np.sum(beta ** 2)\n",
    "    return mse + penalty\n",
    "\n",
    "\n",
    "beta_0 = np.zeros(shape=(x.shape[1],))\n",
    "result = minimize(f, beta_0, method='BFGS')\n",
    "print(result)\n"
   ],
   "id": "4e0655aedee05ccf",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  message: Desired error not necessarily achieved due to precision loss.\n",
      "  success: False\n",
      "   status: 2\n",
      "      fun: 4095.9387337492435\n",
      "        x: [ 1.869e+00 -3.943e-05  2.888e-06  1.031e+01 -8.730e-05\n",
      "            -3.855e-06  3.466e-05  1.423e+00  6.406e-01 -5.678e-05]\n",
      "      nit: 44\n",
      "      jac: [-6.171e-02 -1.086e+01  1.628e+01 -2.887e-02 -8.228e+00\n",
      "            -1.065e+01  2.563e+00  1.318e-02  6.421e-02 -3.080e-01]\n",
      " hess_inv: [[ 5.202e-02 -2.913e-05 ... -1.341e-03 -8.904e-03]\n",
      "            [-2.913e-05  7.300e-07 ...  3.534e-06 -9.934e-05]\n",
      "            ...\n",
      "            [-1.341e-03  3.534e-06 ...  3.544e-02  6.536e-03]\n",
      "            [-8.904e-03 -9.934e-05 ...  6.536e-03  1.052e-01]]\n",
      "     nfev: 2088\n",
      "     njev: 189\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-07T12:46:16.621463Z",
     "start_time": "2025-01-07T12:46:12.497473Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.linear_model import ElasticNet\n",
    "\n",
    "\n",
    "# Try SAGA algorithm\n",
    "\n",
    "def f_enet(x, beta):\n",
    "    # enet criterion\n",
    "    y_hat = x @ beta\n",
    "    mse = np.mean((y - y_hat) ** 2)\n",
    "    penalty = 10.0 * np.sum(np.abs(beta)) + 5 * np.sum(beta ** 2)\n",
    "    return mse + penalty\n",
    "\n",
    "\n",
    "def f_grad_enet(x, beta, dx=1e-9):\n",
    "    n = len(beta)\n",
    "    grad = np.zeros(n)\n",
    "    for i in range(n):\n",
    "        beta_plus = beta.copy()\n",
    "        beta_plus[i] += dx\n",
    "        beta_minus = beta.copy()\n",
    "        beta_minus[i] -= dx\n",
    "        grad[i] = (f_enet(x, beta_plus) - f_enet(x, beta_minus)) / (2 * dx)\n",
    "    return grad\n",
    "\n",
    "\n",
    "def saga_enet(l1: float, l2: float, tol: float = 1e-6, max_iter: int = 50_000) -> np.ndarray:\n",
    "    n, p = x.shape\n",
    "    grad = np.zeros_like(x)\n",
    "    g = np.zeros(x.shape[1])\n",
    "    beta = np.zeros(x.shape[1])\n",
    "    lr = 1e-1\n",
    "    lr_target = 1e-8\n",
    "    lr_factor = (lr_target / lr) ** (1 / max_iter)\n",
    "    n_print = max_iter // 10\n",
    "\n",
    "    for i in range(max_iter):\n",
    "        idx = np.random.randint(n)\n",
    "        prev = grad[idx, :]\n",
    "        g -= prev\n",
    "        x_i = x[idx, :]\n",
    "        y_i = y[idx]\n",
    "        grad_i = (\n",
    "                     # MSE grad\n",
    "                         (x_i @ beta - y_i) * x_i\n",
    "                         # L1 grad\n",
    "                         + l1 * np.sign(beta)\n",
    "                         # L2 grad\n",
    "                         + l2 * beta\n",
    "                 ) / n\n",
    "        grad[idx, :] = grad_i\n",
    "        g += grad_i\n",
    "        beta -= g * lr\n",
    "\n",
    "        if np.linalg.norm(g) < tol:\n",
    "            print(f\"Converged at iteration {i:,}\")\n",
    "            break\n",
    "\n",
    "        if i % n_print == 0:\n",
    "            print(f\"Iteration {i:,} grad norm: {np.linalg.norm(g):.2e}, lr: {lr:.2e}\")\n",
    "        lr *= lr_factor\n",
    "\n",
    "    return beta\n",
    "\n",
    "\n",
    "for l1, l2 in product([0.1, 1.0, 10.0], [0.1, 1.0, 10.0]):\n",
    "    beta_saga = saga_enet(l1, l2)\n",
    "    beta_cd = ElasticNet(alpha=l1 + l2, l1_ratio=l1 / (l1 + l2), fit_intercept=False).fit(x, y).coef_\n",
    "\n",
    "    print(f\"\\nl1={l1}, l2={l2}, diff={np.linalg.norm(beta_saga - beta_cd)/np.linalg.norm(beta_cd):.3%}\")\n",
    "    for b, name in zip([beta_saga, beta_cd], [\"SAGA\", \"CD\"]):\n",
    "        print(f\"{name: >10}: {[round(float(b_), 3) for b_ in b]}\")\n",
    "\n",
    "# beta = saga_enet()\n",
    "# print([round(float(b), 3) for b in beta])\n"
   ],
   "id": "5ecc1ef72c5f7337",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0 grad norm: 9.21e+00, lr: 1.00e-01\n",
      "Iteration 5,000 grad norm: 3.74e-03, lr: 2.00e-02\n",
      "Converged at iteration 7,362\n",
      "\n",
      "l1=0.1, l2=0.1, diff=0.003%\n",
      "      SAGA: [15.193, 0.399, 0.164, 57.463, 1.273, 0.205, -0.313, 8.553, 1.007, -1.499]\n",
      "        CD: [15.193, 0.4, 0.163, 57.463, 1.273, 0.206, -0.314, 8.552, 1.007, -1.499]\n",
      "Iteration 0 grad norm: 2.22e+00, lr: 1.00e-01\n",
      "Iteration 5,000 grad norm: 5.08e-05, lr: 2.00e-02\n",
      "Converged at iteration 5,917\n",
      "\n",
      "l1=0.1, l2=1.0, diff=0.001%\n",
      "      SAGA: [8.356, 0.122, -0.56, 32.263, 0.15, 0.264, 0.839, 5.6, 2.438, -1.725]\n",
      "        CD: [8.356, 0.121, -0.56, 32.263, 0.15, 0.264, 0.839, 5.6, 2.438, -1.725]\n",
      "Iteration 0 grad norm: 2.83e-02, lr: 1.00e-01\n",
      "Iteration 5,000 grad norm: 2.18e-04, lr: 2.00e-02\n",
      "Converged at iteration 6,346\n",
      "\n",
      "l1=0.1, l2=10.0, diff=0.000%\n",
      "      SAGA: [1.487, 0.024, -0.291, 6.081, -0.093, 0.037, 0.371, 1.247, 0.847, -0.465]\n",
      "        CD: [1.487, 0.024, -0.291, 6.081, -0.093, 0.037, 0.371, 1.247, 0.847, -0.465]\n",
      "Iteration 0 grad norm: 2.14e-01, lr: 1.00e-01\n",
      "Iteration 5,000 grad norm: 2.54e-01, lr: 2.00e-02\n",
      "Iteration 10,000 grad norm: 6.43e-02, lr: 3.98e-03\n",
      "Iteration 15,000 grad norm: 7.54e-02, lr: 7.94e-04\n",
      "Iteration 20,000 grad norm: 7.85e-02, lr: 1.58e-04\n",
      "Iteration 25,000 grad norm: 7.56e-02, lr: 3.16e-05\n",
      "Iteration 30,000 grad norm: 5.10e-02, lr: 6.31e-06\n",
      "Iteration 35,000 grad norm: 1.35e-01, lr: 1.26e-06\n",
      "Iteration 40,000 grad norm: 7.96e-02, lr: 2.51e-07\n",
      "Iteration 45,000 grad norm: 1.84e-01, lr: 5.01e-08\n",
      "\n",
      "l1=1.0, l2=0.1, diff=0.000%\n",
      "      SAGA: [14.224, 0.0, 0.0, 56.846, 0.394, 0.0, 0.0, 7.917, 0.237, -0.499]\n",
      "        CD: [14.224, 0.0, -0.0, 56.846, 0.395, -0.0, -0.0, 7.917, 0.237, -0.499]\n",
      "Iteration 0 grad norm: 2.18e+00, lr: 1.00e-01\n",
      "Iteration 5,000 grad norm: 8.17e-02, lr: 2.00e-02\n",
      "Iteration 10,000 grad norm: 2.07e-01, lr: 3.98e-03\n",
      "Iteration 15,000 grad norm: 1.24e-01, lr: 7.94e-04\n",
      "Iteration 20,000 grad norm: 8.63e-02, lr: 1.58e-04\n",
      "Iteration 25,000 grad norm: 1.58e-01, lr: 3.16e-05\n",
      "Iteration 30,000 grad norm: 1.42e-01, lr: 6.31e-06\n",
      "Iteration 35,000 grad norm: 7.43e-02, lr: 1.26e-06\n",
      "Iteration 40,000 grad norm: 6.62e-02, lr: 2.51e-07\n",
      "Iteration 45,000 grad norm: 3.55e-02, lr: 5.01e-08\n",
      "\n",
      "l1=1.0, l2=1.0, diff=0.001%\n",
      "      SAGA: [7.827, -0.0, -0.165, 31.934, 0.0, 0.0, 0.402, 5.194, 2.058, -1.235]\n",
      "        CD: [7.827, 0.0, -0.165, 31.934, 0.0, 0.0, 0.403, 5.194, 2.058, -1.235]\n",
      "Iteration 0 grad norm: 1.43e+00, lr: 1.00e-01\n",
      "Iteration 5,000 grad norm: 9.60e-02, lr: 2.00e-02\n",
      "Iteration 10,000 grad norm: 4.60e-02, lr: 3.98e-03\n",
      "Iteration 15,000 grad norm: 7.63e-02, lr: 7.94e-04\n",
      "Iteration 20,000 grad norm: 5.56e-03, lr: 1.58e-04\n",
      "Iteration 25,000 grad norm: 6.61e-02, lr: 3.16e-05\n",
      "Iteration 30,000 grad norm: 2.01e-02, lr: 6.31e-06\n",
      "Iteration 35,000 grad norm: 3.98e-02, lr: 1.26e-06\n",
      "Iteration 40,000 grad norm: 8.61e-02, lr: 2.51e-07\n",
      "Iteration 45,000 grad norm: 1.31e-01, lr: 5.01e-08\n",
      "\n",
      "l1=1.0, l2=10.0, diff=0.000%\n",
      "      SAGA: [1.402, -0.0, -0.211, 6.004, -0.011, 0.0, 0.291, 1.168, 0.769, -0.382]\n",
      "        CD: [1.402, 0.0, -0.211, 6.004, -0.011, 0.0, 0.291, 1.168, 0.769, -0.382]\n",
      "Iteration 0 grad norm: 2.78e-01, lr: 1.00e-01\n",
      "Iteration 5,000 grad norm: 1.54e+00, lr: 2.00e-02\n",
      "Iteration 10,000 grad norm: 2.24e+00, lr: 3.98e-03\n",
      "Iteration 15,000 grad norm: 1.67e+00, lr: 7.94e-04\n",
      "Iteration 20,000 grad norm: 1.91e+00, lr: 1.58e-04\n",
      "Iteration 25,000 grad norm: 9.87e-01, lr: 3.16e-05\n",
      "Iteration 30,000 grad norm: 1.53e+00, lr: 6.31e-06\n",
      "Iteration 35,000 grad norm: 2.40e+00, lr: 1.26e-06\n",
      "Iteration 40,000 grad norm: 2.49e+00, lr: 2.51e-07\n",
      "Iteration 45,000 grad norm: 1.69e+00, lr: 5.01e-08\n",
      "\n",
      "l1=10.0, l2=0.1, diff=0.001%\n",
      "      SAGA: [3.894, 0.0, -0.0, 50.158, 0.0, -0.0, -0.0, 0.209, 0.0, -0.0]\n",
      "        CD: [3.894, 0.0, -0.0, 50.158, 0.0, -0.0, -0.0, 0.209, 0.0, -0.0]\n",
      "Iteration 0 grad norm: 3.39e+00, lr: 1.00e-01\n",
      "Iteration 5,000 grad norm: 2.26e+00, lr: 2.00e-02\n",
      "Iteration 10,000 grad norm: 1.43e+00, lr: 3.98e-03\n",
      "Iteration 15,000 grad norm: 1.47e+00, lr: 7.94e-04\n",
      "Iteration 20,000 grad norm: 1.49e+00, lr: 1.58e-04\n",
      "Iteration 25,000 grad norm: 2.30e+00, lr: 3.16e-05\n",
      "Iteration 30,000 grad norm: 2.02e+00, lr: 6.31e-06\n",
      "Iteration 35,000 grad norm: 2.17e+00, lr: 1.26e-06\n",
      "Iteration 40,000 grad norm: 1.55e+00, lr: 2.51e-07\n",
      "Iteration 45,000 grad norm: 1.18e+00, lr: 5.01e-08\n",
      "\n",
      "l1=10.0, l2=1.0, diff=0.001%\n",
      "      SAGA: [2.683, 0.0, -0.0, 28.083, 0.0, -0.0, 0.0, 1.038, 0.0, -0.0]\n",
      "        CD: [2.683, 0.0, -0.0, 28.084, 0.0, -0.0, 0.0, 1.038, 0.0, -0.0]\n",
      "Iteration 0 grad norm: 2.71e+00, lr: 1.00e-01\n",
      "Iteration 5,000 grad norm: 1.54e+00, lr: 2.00e-02\n",
      "Iteration 10,000 grad norm: 1.30e+00, lr: 3.98e-03\n",
      "Iteration 15,000 grad norm: 1.96e+00, lr: 7.94e-04\n",
      "Iteration 20,000 grad norm: 2.05e+00, lr: 1.58e-04\n",
      "Iteration 25,000 grad norm: 9.73e-01, lr: 3.16e-05\n",
      "Iteration 30,000 grad norm: 1.43e+00, lr: 6.31e-06\n",
      "Iteration 35,000 grad norm: 1.58e+00, lr: 1.26e-06\n",
      "Iteration 40,000 grad norm: 1.75e+00, lr: 2.51e-07\n",
      "Iteration 45,000 grad norm: 2.12e+00, lr: 5.01e-08\n",
      "\n",
      "l1=10.0, l2=10.0, diff=0.000%\n",
      "      SAGA: [0.562, 0.0, -0.0, 5.217, -0.0, -0.0, 0.0, 0.368, 0.0, -0.0]\n",
      "        CD: [0.562, 0.0, -0.0, 5.217, -0.0, 0.0, 0.0, 0.368, 0.0, -0.0]\n"
     ]
    }
   ],
   "execution_count": 227
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-07T12:01:11.010855Z",
     "start_time": "2025-01-07T12:01:10.990321Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def adam_enet(beta_1: float = 0.98, beta_2: float = 0.99, max_iter: int = 200_000) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Minimize the Elastic Net criterion using the Adam optimizer.\n",
    "\n",
    "    Args:\n",
    "      beta_1: Exponential decay rate for the first moment estimates.\n",
    "      beta_2: Exponential decay rate for the second moment estimates.\n",
    "      max_iter: Maximum number of iterations.\n",
    "\n",
    "    Returns:\n",
    "      An array of the estimated coefficients.\n",
    "    \"\"\"\n",
    "    n, p = x.shape\n",
    "    beta = np.zeros(p)\n",
    "    m = np.zeros(p)\n",
    "    v = np.ones(p)\n",
    "    epsilon = 1e-8\n",
    "    alpha = 0.001  # Learning rate\n",
    "\n",
    "    for t in range(1, max_iter + 1):\n",
    "        batch_idx = np.random.randint(n, size=32)\n",
    "        batch = x[batch_idx, :]\n",
    "        grad = f_grad_enet(batch, beta) / n\n",
    "        m = beta_1 * m + (1 - beta_1) * grad\n",
    "        v = beta_2 * v + (1 - beta_2) * np.square(grad)\n",
    "        m_hat = m / (1 - beta_1 ** t)\n",
    "        v_hat = v / (1 - beta_2 ** t)\n",
    "        beta -= alpha * m_hat / (np.sqrt(v_hat) + epsilon)\n",
    "\n",
    "        if t % (max_iter // 10) == 0:\n",
    "            print(f\"Iteration {t:,} grad norm: {np.linalg.norm(grad)}\")\n",
    "\n",
    "    return beta\n",
    "\n",
    "\n",
    "beta = adam_enet()\n",
    "print(np.round(beta, 3))"
   ],
   "id": "3a2369632b232c7f",
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (100,) (32,) ",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[118], line 35\u001B[0m\n\u001B[1;32m     31\u001B[0m             \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mIteration \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mt\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m,\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m grad norm: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mnp\u001B[38;5;241m.\u001B[39mlinalg\u001B[38;5;241m.\u001B[39mnorm(grad)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     33\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m beta\n\u001B[0;32m---> 35\u001B[0m beta \u001B[38;5;241m=\u001B[39m \u001B[43madam_enet\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     36\u001B[0m \u001B[38;5;28mprint\u001B[39m(np\u001B[38;5;241m.\u001B[39mround(beta, \u001B[38;5;241m3\u001B[39m))\n",
      "Cell \u001B[0;32mIn[118], line 23\u001B[0m, in \u001B[0;36madam_enet\u001B[0;34m(beta_1, beta_2, max_iter)\u001B[0m\n\u001B[1;32m     21\u001B[0m batch_idx \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mrandom\u001B[38;5;241m.\u001B[39mrandint(n, size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m32\u001B[39m)\n\u001B[1;32m     22\u001B[0m batch \u001B[38;5;241m=\u001B[39m x[batch_idx, :]\n\u001B[0;32m---> 23\u001B[0m grad \u001B[38;5;241m=\u001B[39m \u001B[43mf_grad_enet\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbeta\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;241m/\u001B[39m n\n\u001B[1;32m     24\u001B[0m m \u001B[38;5;241m=\u001B[39m beta_1 \u001B[38;5;241m*\u001B[39m m \u001B[38;5;241m+\u001B[39m (\u001B[38;5;241m1\u001B[39m \u001B[38;5;241m-\u001B[39m beta_1) \u001B[38;5;241m*\u001B[39m grad\n\u001B[1;32m     25\u001B[0m v \u001B[38;5;241m=\u001B[39m beta_2 \u001B[38;5;241m*\u001B[39m v \u001B[38;5;241m+\u001B[39m (\u001B[38;5;241m1\u001B[39m \u001B[38;5;241m-\u001B[39m beta_2) \u001B[38;5;241m*\u001B[39m np\u001B[38;5;241m.\u001B[39msquare(grad)\n",
      "Cell \u001B[0;32mIn[108], line 18\u001B[0m, in \u001B[0;36mf_grad_enet\u001B[0;34m(x, beta, dx)\u001B[0m\n\u001B[1;32m     16\u001B[0m     beta_minus \u001B[38;5;241m=\u001B[39m beta\u001B[38;5;241m.\u001B[39mcopy()\n\u001B[1;32m     17\u001B[0m     beta_minus[i] \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m=\u001B[39m dx\n\u001B[0;32m---> 18\u001B[0m     grad[i] \u001B[38;5;241m=\u001B[39m (\u001B[43mf_enet\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbeta_plus\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;241m-\u001B[39m f_enet(x, beta_minus)) \u001B[38;5;241m/\u001B[39m (\u001B[38;5;241m2\u001B[39m \u001B[38;5;241m*\u001B[39m dx)\n\u001B[1;32m     19\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m grad\n",
      "Cell \u001B[0;32mIn[108], line 6\u001B[0m, in \u001B[0;36mf_enet\u001B[0;34m(x, beta)\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mf_enet\u001B[39m(x, beta):\n\u001B[1;32m      4\u001B[0m     \u001B[38;5;66;03m# enet criterion\u001B[39;00m\n\u001B[1;32m      5\u001B[0m     y_hat \u001B[38;5;241m=\u001B[39m x \u001B[38;5;241m@\u001B[39m beta\n\u001B[0;32m----> 6\u001B[0m     mse \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mmean((\u001B[43my\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m-\u001B[39;49m\u001B[43m \u001B[49m\u001B[43my_hat\u001B[49m)\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m2\u001B[39m)\n\u001B[1;32m      7\u001B[0m     penalty \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m10.0\u001B[39m \u001B[38;5;241m*\u001B[39m np\u001B[38;5;241m.\u001B[39msum(np\u001B[38;5;241m.\u001B[39mabs(beta)) \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m5\u001B[39m \u001B[38;5;241m*\u001B[39m np\u001B[38;5;241m.\u001B[39msum(beta\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m2\u001B[39m)\n\u001B[1;32m      8\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m mse \u001B[38;5;241m+\u001B[39m penalty\n",
      "\u001B[0;31mValueError\u001B[0m: operands could not be broadcast together with shapes (100,) (32,) "
     ]
    }
   ],
   "execution_count": 118
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
