{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "from collections import deque\n",
    "from itertools import product\n",
    "\n",
    "from math import inf\n",
    "import numpy as np\n",
    "import scipy.special\n",
    "from sklearn.datasets import make_regression\n",
    "\n",
    "\n",
    "def bfgs(f, f_grad, x0, max_iter=500, tol=1e-6):\n",
    "    \"\"\"\n",
    "    BFGS algorithm for unconstrained optimization.\n",
    "\n",
    "    Args:\n",
    "        f: The objective function to be minimized.\n",
    "        f_grad: The gradient of the objective function.\n",
    "        x0: The initial guess for the solution.\n",
    "        max_iter: The maximum number of iterations.\n",
    "        tol: The tolerance for convergence.\n",
    "\n",
    "    Returns:\n",
    "        A tuple containing:\n",
    "            - The optimal solution found.\n",
    "            - The value of the objective function at the solution.\n",
    "            - The number of iterations performed.\n",
    "    \"\"\"\n",
    "\n",
    "    x = x0\n",
    "    n = len(x0)\n",
    "    B = np.eye(n)  # Initial approximation of the Hessian inverse\n",
    "    grad = f_grad(x)\n",
    "\n",
    "    for i in range(max_iter):\n",
    "        # Compute search direction\n",
    "        p = -np.linalg.solve(B, grad)\n",
    "\n",
    "        # Perform line search\n",
    "        t = line_search(f, f_grad, x, p)\n",
    "\n",
    "        # Update x\n",
    "        x_next = x + t * p\n",
    "\n",
    "        # Update B (BFGS update)\n",
    "        s = x_next - x\n",
    "        y = f_grad(x_next) - grad\n",
    "        if np.dot(s, y) > 0:  # Ensure positive definiteness\n",
    "            rho = 1 / np.dot(s, y)\n",
    "            B = (np.eye(n) - rho * np.outer(s, y)) @ B @ (np.eye(n) - rho * np.outer(y, s)) + rho * np.outer(s, s)\n",
    "        else:\n",
    "            B = np.eye(n)\n",
    "\n",
    "        # Update x and gradient\n",
    "        x = x_next\n",
    "        grad = f_grad(x)\n",
    "\n",
    "        # Check for convergence\n",
    "        if np.linalg.norm(grad) < tol:\n",
    "            break\n",
    "\n",
    "    return x, f(x), i + 1\n",
    "\n",
    "\n",
    "def line_search(f, f_grad, x, p, c1=1e-4, c2=0.9):\n",
    "    \"\"\"\n",
    "    Backtracking line search.\n",
    "\n",
    "    Args:\n",
    "        f: The objective function.\n",
    "        f_grad: The gradient of the objective function.\n",
    "        x: The current point.\n",
    "        p: The search direction.\n",
    "        c1: The Armijo condition parameter.\n",
    "        c2: The curvature condition parameter.\n",
    "\n",
    "    Returns:\n",
    "        The step size that satisfies the Wolfe conditions.\n",
    "    \"\"\"\n",
    "    assert 0 < c1 < c2 < 1\n",
    "    beta = inf\n",
    "    alpha = 0.0\n",
    "    t = 1.0\n",
    "    s = np.dot(f_grad(x), p)\n",
    "\n",
    "    for iter in range(10_000):\n",
    "        # Check Armijo Condition\n",
    "        if f(x + t * p) - f(x) <= c1 * t * s:\n",
    "            # Check Wolfe Condition\n",
    "            if f_grad(x + t * p) @ p >= c2 * s:\n",
    "                break\n",
    "            else:\n",
    "                # Wolfe failed\n",
    "                alpha = t\n",
    "        else:\n",
    "            # Armijo failed\n",
    "            beta = t\n",
    "        if beta < inf:\n",
    "            t = (alpha + beta) / 2\n",
    "        else:\n",
    "            t = 2 * alpha\n",
    "\n",
    "    if iter == 10_000 - 1:\n",
    "        raise ValueError(\"Line search did not converge at x={x}, p={p}, t={t:.2e}\".format(x=x, p=p, t=t))\n",
    "    return t\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "def f(x):\n",
    "    return x[0] ** 2 + 5 * x[1] ** 4 + 0.1 * abs(x[0])\n",
    "\n",
    "\n",
    "def f_grad(x, dx=1e-6):\n",
    "    n = len(x)\n",
    "    grad = np.zeros(n)\n",
    "    f_val = f(x)\n",
    "    for i in range(n):\n",
    "        x_plus = x.copy()\n",
    "        x_plus[i] += dx\n",
    "        grad[i] = (f(x_plus) - f_val) / dx\n",
    "    return grad\n",
    "\n",
    "\n",
    "x0 = np.array([3.0, 4.0])\n",
    "x_opt, f_opt, iterations = bfgs(f, f_grad, x0)\n",
    "\n",
    "print(\"Optimal solution:\", x_opt)\n",
    "print(\"Optimal value:\", f_opt)\n",
    "print(\"Iterations:\", iterations)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from scipy.optimize import minimize, minimize_scalar\n",
    "from sklearn.datasets import make_regression\n",
    "\n",
    "x, y = make_regression(n_samples=100, n_features=10, noise=10, n_informative=3, random_state=42)\n",
    "\n",
    "\n",
    "def f(beta):\n",
    "    # enet criterion\n",
    "    y_hat = x @ beta\n",
    "    mse = np.mean((y - y_hat) ** 2)\n",
    "    penalty = 10.0 * np.sum(np.abs(beta)) + 5 * np.sum(beta ** 2)\n",
    "    return mse + penalty\n",
    "\n",
    "\n",
    "beta_0 = np.zeros(shape=(x.shape[1],))\n",
    "result = minimize(f, beta_0, method='BFGS')\n",
    "print(result)\n"
   ],
   "id": "4e0655aedee05ccf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# GD with line search Enet\n",
    "from scipy.optimize import approx_fprime, minimize_scalar, minimize\n",
    "from collections import deque\n",
    "\n",
    "\n",
    "def f_enet(beta, l1: float, l2: float) -> float:\n",
    "    # enet criterion\n",
    "    y_hat = x @ beta\n",
    "    mse = np.mean((y - y_hat) ** 2)\n",
    "    penalty = l1 * np.sum(np.abs(beta)) + l2 * np.sum(np.square(beta))\n",
    "    return mse + penalty\n",
    "\n",
    "\n",
    "def f_grad_enet(beta, l1: float, l2: float) -> np.ndarray:\n",
    "    grad_mse = x.T @ (x @ beta - y)\n",
    "    grad_penalty = l1 * np.sign(beta) + l2 * beta\n",
    "    return (grad_mse + grad_penalty) / x.shape[0]\n",
    "\n",
    "\n",
    "def gd_with_line_search(l1: float, l2: float, max_iter=1_000, loss_rtol: float = 1e-8) -> np.ndarray:\n",
    "    n, p = x.shape\n",
    "    beta = np.zeros(p, dtype=np.float64)\n",
    "    loss = f_enet(beta, l1, l2)\n",
    "    g = np.zeros_like(beta)\n",
    "    print_every = max_iter // 5\n",
    "    grad_buffer = deque(maxlen=2)\n",
    "    grad_buffer.append(np.zeros_like(beta))\n",
    "    grad_buffer.append(np.zeros_like(beta))\n",
    "\n",
    "    for i in range(max_iter):\n",
    "        grad = f_grad_enet(beta, l1, l2)\n",
    "        grad_buffer.append(grad)\n",
    "\n",
    "        # Perform line search and convex combination of grads\n",
    "        def h(x):\n",
    "            t, alpha = x\n",
    "            p = alpha * grad_buffer[0] + (1 - alpha) * grad_buffer[1]\n",
    "            return f_enet(beta - t * p, l1, l2)\n",
    "\n",
    "        res = minimize(h, [0.5, 0.5], bounds=[(1e-10, 1), (0, 1)])\n",
    "        t, alpha = res.x\n",
    "        p = alpha * grad_buffer[0] + (1 - alpha) * grad_buffer[1]\n",
    "        new_beta = beta - t * p\n",
    "        new_loss = f_enet(new_beta, l1, l2)\n",
    "\n",
    "        if i % print_every == 0:\n",
    "            print(\n",
    "                f\"Iteration {i:,}, loss {new_loss:.2e}, t {t:.2e}, alpha {alpha:.2f}, grad norm {np.linalg.norm(grad):.2e}\")\n",
    "\n",
    "        beta = new_beta\n",
    "        loss = new_loss\n",
    "\n",
    "    return beta\n",
    "\n",
    "\n",
    "# [0.562, -0.0, 0.0, 5.217, -0.0, -0.0, 0.0, 0.368, 0.0, -0.0]\n",
    "b = gd_with_line_search(10.0, 10.0, max_iter=100)\n",
    "print([round(float(b_), 3) for b_ in b])\n",
    "\n",
    "b_ridge = np.linalg.solve(x.T @ x + 1000 * np.eye(x.shape[1]), x.T @ y)\n",
    "print([round(float(b_), 3) for b_ in b_ridge])\n"
   ],
   "id": "3a501bf6c278a63b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "7adb8c3b8b27c94",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def sgd_with_momentum(l1: float, l2: float, max_iter=1_000, mom_span=100, target_lr=1e-7) -> np.ndarray:\n",
    "    n, p = x.shape\n",
    "    beta = np.zeros(p, dtype=np.float64)\n",
    "    loss = f_enet(beta, l1, l2)\n",
    "    g = np.zeros_like(beta)\n",
    "    t = 0.1\n",
    "    decay = (target_lr / t) ** (1 / max_iter)\n",
    "    print_every = max_iter // 5\n",
    "    mom = 1 - 2 / mom_span\n",
    "\n",
    "    for i in range(max_iter):\n",
    "        # Sample a random data point\n",
    "        idx = np.random.randint(0, n)\n",
    "        x_i = x[idx]\n",
    "        y_i = y[idx]\n",
    "\n",
    "        # Calculate the gradient for the sampled data point\n",
    "        grad = x_i * (x_i @ beta - y_i)  # Gradient of MSE for the data point\n",
    "        grad_penalty = l1 * np.sign(beta) + l2 * beta  # Gradient of penalty\n",
    "        grad += grad_penalty  # Add the penalty gradient\n",
    "\n",
    "        g = (1 - mom) * grad + mom * g  # Update momentum\n",
    "        p = -g  # Update direction\n",
    "\n",
    "        new_beta = beta + t * p\n",
    "        new_loss = f_enet(new_beta, l1, l2)\n",
    "\n",
    "        if i % print_every == 0:\n",
    "            print(f\"Iteration {i:,}, loss {new_loss:.2e}, t {t:.2e}\")\n",
    "\n",
    "        beta = new_beta\n",
    "        loss = new_loss\n",
    "        t *= decay\n",
    "\n",
    "    return beta\n",
    "\n",
    "\n",
    "# [0.562, -0.0, 0.0, 5.217, -0.0, -0.0, 0.0, 0.368, 0.0, -0.0]\n",
    "b = sgd_with_momentum(10, 10, max_iter=50_000, mom_span=300, target_lr=1e-8)\n",
    "print([round(float(b_), 3) for b_ in b])\n"
   ],
   "id": "a42befae76f0c173",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.linear_model import ElasticNet\n",
    "\n",
    "\n",
    "def saga_enet(l1: float, l2: float, tol: float = 1e-6, max_iter: int = 50_000) -> np.ndarray:\n",
    "    n, p = x.shape\n",
    "    grad = np.zeros_like(x)\n",
    "    g = np.zeros(x.shape[1])\n",
    "    beta = np.zeros(x.shape[1])\n",
    "    lr = 1e-1\n",
    "    lr_target = 1e-10\n",
    "    lr_factor = (lr_target / lr) ** (1 / max_iter)\n",
    "    n_print = max_iter // 10\n",
    "\n",
    "    for i in range(max_iter):\n",
    "        idx = np.random.randint(n)\n",
    "        prev = grad[idx, :]\n",
    "        g -= prev\n",
    "        x_i = x[idx, :]\n",
    "        y_i = y[idx]\n",
    "        grad_i = (\n",
    "                     # MSE grad\n",
    "                         (x_i @ beta - y_i) * x_i\n",
    "                         # L1 grad\n",
    "                         + l1 * np.sign(beta)\n",
    "                         # L2 grad\n",
    "                         + l2 * beta\n",
    "                 ) / n\n",
    "        grad[idx, :] = grad_i\n",
    "        g += grad_i\n",
    "        beta -= g * lr\n",
    "\n",
    "        if np.linalg.norm(g) < tol:\n",
    "            print(f\"Converged at iteration {i:,}\")\n",
    "            break\n",
    "\n",
    "        if i % n_print == 0:\n",
    "            print(f\"Iteration {i:,} grad norm: {np.linalg.norm(g):.2e}, lr: {lr:.2e}\")\n",
    "        lr *= lr_factor\n",
    "\n",
    "    return beta\n",
    "\n",
    "\n",
    "for l1, l2 in product([0.1, 1.0, 10.0], [0.1, 1.0, 10.0]):\n",
    "    beta_saga = saga_enet(l1, l2)\n",
    "    beta_cd = ElasticNet(alpha=l1 + l2, l1_ratio=l1 / (l1 + l2), fit_intercept=False).fit(x, y).coef_\n",
    "    beta_gd = gd_with_line_search(l1, l2)\n",
    "\n",
    "    print(f\"\\nl1={l1}, l2={l2}, diff={np.linalg.norm(beta_saga - beta_cd) / np.linalg.norm(beta_cd):.3%}\")\n",
    "    for b, name in zip([beta_saga, beta_cd, beta_gd], [\"SAGA\", \"CD\", \"GD + LS\"]):\n",
    "        print(f\"{name: >10}: {[round(float(b_), 3) for b_ in b]}\")\n",
    "\n",
    "# beta = saga_enet()\n",
    "# print([round(float(b), 3) for b in beta])\n"
   ],
   "id": "5ecc1ef72c5f7337",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-10T15:19:20.698429Z",
     "start_time": "2025-01-10T15:19:19.993676Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def adam_enet(l1, l2, b1_span: int = 10, b2_span: int = 100, max_iter: int = 200_000) -> np.ndarray:\n",
    "    beta_1 = 1 - 2 / b1_span\n",
    "    beta_2 = 1 - 2 / b2_span\n",
    "    n, p = x.shape\n",
    "    beta = np.zeros(p)\n",
    "    m = np.zeros(p)\n",
    "    v = np.ones(p)\n",
    "    epsilon = 1e-8\n",
    "    alpha = 0.1\n",
    "    target_lr = 1e-9\n",
    "    decay = (target_lr / alpha) ** (1 / max_iter)\n",
    "    print_every = max_iter // 3\n",
    "\n",
    "    for t in range(1, max_iter + 1):\n",
    "        i = np.random.randint(n)\n",
    "\n",
    "        x_i = x[i]\n",
    "        y_i = y[i]\n",
    "        mse_grad = x_i * (x_i @ beta - y_i)\n",
    "        penalty_grad = l1 * np.sign(beta) + l2 * beta\n",
    "        grad = mse_grad + penalty_grad\n",
    "\n",
    "        m = beta_1 * m + (1 - beta_1) * grad\n",
    "        v = beta_2 * v + (1 - beta_2) * np.square(grad - m)\n",
    "\n",
    "        p = -alpha * (m / (np.sqrt(v) + epsilon))\n",
    "        beta += p\n",
    "\n",
    "        if t % print_every == 0:\n",
    "            loss = f_enet(beta, l1, l2)\n",
    "            print(f\"Iteration {t:,}, loss {loss:.2e}\")\n",
    "\n",
    "        alpha *= decay\n",
    "\n",
    "    return beta\n",
    "\n",
    "\n",
    "n, p = x.shape\n",
    "# [0.562, -0.0, 0.0, 5.217, -0.0, -0.0, 0.0, 0.368, 0.0, -0.0]\n",
    "b = adam_enet(10.0, 10.0, b1_span=10, b2_span=100, max_iter=80_000)\n",
    "print([round(float(b_), 3) for b_ in b])"
   ],
   "id": "3a2369632b232c7f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 26,666, loss 4.42e+03\n",
      "Iteration 53,332, loss 4.42e+03\n",
      "Iteration 79,998, loss 4.42e+03\n",
      "[0.568, -0.0, -0.0, 5.052, -0.0, -0.0, 0.0, 0.308, 0.059, 0.0]\n"
     ]
    }
   ],
   "execution_count": 175
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-10T15:37:07.351976Z",
     "start_time": "2025-01-10T15:37:07.346292Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.linear_model import SGDRegressor\n",
    "\n",
    "l1 = 10.0\n",
    "l2 = 10.0\n",
    "reg = SGDRegressor(penalty='elasticnet', alpha=l1 + l2, l1_ratio=l1 / (l1 + l2), max_iter=10_000, tol=1e-6,\n",
    "                   validation_fraction=0.0001, fit_intercept=False)\n",
    "reg.fit(x, y)\n",
    "\n",
    "print([round(float(b_), 3) for b_ in reg.coef_])"
   ],
   "id": "9418c274bbbf5af1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.82, 0.0, 0.0, 5.285, 0.0, 0.0, 0.0, 0.865, 0.0, 0.0]\n"
     ]
    }
   ],
   "execution_count": 306
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "526d81c00bfbce74"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
