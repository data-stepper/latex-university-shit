{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-01-09T13:37:06.395988Z",
     "start_time": "2025-01-09T13:37:06.312689Z"
    }
   },
   "source": [
    "from itertools import product\n",
    "\n",
    "from math import inf\n",
    "import numpy as np\n",
    "import scipy.special\n",
    "from sklearn.datasets import make_regression\n",
    "\n",
    "\n",
    "def bfgs(f, f_grad, x0, max_iter=500, tol=1e-6):\n",
    "    \"\"\"\n",
    "    BFGS algorithm for unconstrained optimization.\n",
    "\n",
    "    Args:\n",
    "        f: The objective function to be minimized.\n",
    "        f_grad: The gradient of the objective function.\n",
    "        x0: The initial guess for the solution.\n",
    "        max_iter: The maximum number of iterations.\n",
    "        tol: The tolerance for convergence.\n",
    "\n",
    "    Returns:\n",
    "        A tuple containing:\n",
    "            - The optimal solution found.\n",
    "            - The value of the objective function at the solution.\n",
    "            - The number of iterations performed.\n",
    "    \"\"\"\n",
    "\n",
    "    x = x0\n",
    "    n = len(x0)\n",
    "    B = np.eye(n)  # Initial approximation of the Hessian inverse\n",
    "    grad = f_grad(x)\n",
    "\n",
    "    for i in range(max_iter):\n",
    "        # Compute search direction\n",
    "        p = -np.linalg.solve(B, grad)\n",
    "\n",
    "        # Perform line search\n",
    "        t = line_search(f, f_grad, x, p)\n",
    "\n",
    "        # Update x\n",
    "        x_next = x + t * p\n",
    "\n",
    "        # Update B (BFGS update)\n",
    "        s = x_next - x\n",
    "        y = f_grad(x_next) - grad\n",
    "        if np.dot(s, y) > 0:  # Ensure positive definiteness\n",
    "            rho = 1 / np.dot(s, y)\n",
    "            B = (np.eye(n) - rho * np.outer(s, y)) @ B @ (np.eye(n) - rho * np.outer(y, s)) + rho * np.outer(s, s)\n",
    "        else:\n",
    "            B = np.eye(n)\n",
    "\n",
    "        # Update x and gradient\n",
    "        x = x_next\n",
    "        grad = f_grad(x)\n",
    "\n",
    "        # Check for convergence\n",
    "        if np.linalg.norm(grad) < tol:\n",
    "            break\n",
    "\n",
    "    return x, f(x), i + 1\n",
    "\n",
    "\n",
    "def line_search(f, f_grad, x, p, c1=1e-4, c2=0.9):\n",
    "    \"\"\"\n",
    "    Backtracking line search.\n",
    "\n",
    "    Args:\n",
    "        f: The objective function.\n",
    "        f_grad: The gradient of the objective function.\n",
    "        x: The current point.\n",
    "        p: The search direction.\n",
    "        c1: The Armijo condition parameter.\n",
    "        c2: The curvature condition parameter.\n",
    "\n",
    "    Returns:\n",
    "        The step size that satisfies the Wolfe conditions.\n",
    "    \"\"\"\n",
    "    assert 0 < c1 < c2 < 1\n",
    "    beta = inf\n",
    "    alpha = 0.0\n",
    "    t = 1.0\n",
    "    s = np.dot(f_grad(x), p)\n",
    "\n",
    "    for iter in range(10_000):\n",
    "        # Check Armijo Condition\n",
    "        if f(x + t * p) - f(x) <= c1 * t * s:\n",
    "            # Check Wolfe Condition\n",
    "            if f_grad(x + t * p) @ p >= c2 * s:\n",
    "                break\n",
    "            else:\n",
    "                # Wolfe failed\n",
    "                alpha = t\n",
    "        else:\n",
    "            # Armijo failed\n",
    "            beta = t\n",
    "        if beta < inf:\n",
    "            t = (alpha + beta) / 2\n",
    "        else:\n",
    "            t = 2 * alpha\n",
    "\n",
    "    if iter == 10_000 - 1:\n",
    "        raise ValueError(\"Line search did not converge at x={x}, p={p}, t={t:.2e}\".format(x=x, p=p, t=t))\n",
    "    return t\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "def f(x):\n",
    "    return x[0] ** 2 + 5 * x[1] ** 4 + 0.1 * abs(x[0])\n",
    "\n",
    "\n",
    "def f_grad(x, dx=1e-6):\n",
    "    n = len(x)\n",
    "    grad = np.zeros(n)\n",
    "    f_val = f(x)\n",
    "    for i in range(n):\n",
    "        x_plus = x.copy()\n",
    "        x_plus[i] += dx\n",
    "        grad[i] = (f(x_plus) - f_val) / dx\n",
    "    return grad\n",
    "\n",
    "\n",
    "x0 = np.array([3.0, 4.0])\n",
    "x_opt, f_opt, iterations = bfgs(f, f_grad, x0)\n",
    "\n",
    "print(\"Optimal solution:\", x_opt)\n",
    "print(\"Optimal value:\", f_opt)\n",
    "print(\"Iterations:\", iterations)"
   ],
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Line search did not converge at x=[ 4.55014146e-08 -1.22990350e-01], p=[-1.46540516e+04  4.90101639e-02], t=6.21e-12",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[2], line 122\u001B[0m\n\u001B[1;32m    118\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m grad\n\u001B[1;32m    121\u001B[0m x0 \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39marray([\u001B[38;5;241m3.0\u001B[39m, \u001B[38;5;241m4.0\u001B[39m])\n\u001B[0;32m--> 122\u001B[0m x_opt, f_opt, iterations \u001B[38;5;241m=\u001B[39m \u001B[43mbfgs\u001B[49m\u001B[43m(\u001B[49m\u001B[43mf\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mf_grad\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mx0\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    124\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mOptimal solution:\u001B[39m\u001B[38;5;124m\"\u001B[39m, x_opt)\n\u001B[1;32m    125\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mOptimal value:\u001B[39m\u001B[38;5;124m\"\u001B[39m, f_opt)\n",
      "Cell \u001B[0;32mIn[2], line 37\u001B[0m, in \u001B[0;36mbfgs\u001B[0;34m(f, f_grad, x0, max_iter, tol)\u001B[0m\n\u001B[1;32m     34\u001B[0m p \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m-\u001B[39mnp\u001B[38;5;241m.\u001B[39mlinalg\u001B[38;5;241m.\u001B[39msolve(B, grad)\n\u001B[1;32m     36\u001B[0m \u001B[38;5;66;03m# Perform line search\u001B[39;00m\n\u001B[0;32m---> 37\u001B[0m t \u001B[38;5;241m=\u001B[39m \u001B[43mline_search\u001B[49m\u001B[43m(\u001B[49m\u001B[43mf\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mf_grad\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mp\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     39\u001B[0m \u001B[38;5;66;03m# Update x\u001B[39;00m\n\u001B[1;32m     40\u001B[0m x_next \u001B[38;5;241m=\u001B[39m x \u001B[38;5;241m+\u001B[39m t \u001B[38;5;241m*\u001B[39m p\n",
      "Cell \u001B[0;32mIn[2], line 101\u001B[0m, in \u001B[0;36mline_search\u001B[0;34m(f, f_grad, x, p, c1, c2)\u001B[0m\n\u001B[1;32m     98\u001B[0m         t \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m2\u001B[39m \u001B[38;5;241m*\u001B[39m alpha\n\u001B[1;32m    100\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28miter\u001B[39m \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m10_000\u001B[39m \u001B[38;5;241m-\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[0;32m--> 101\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mLine search did not converge at x=\u001B[39m\u001B[38;5;132;01m{x}\u001B[39;00m\u001B[38;5;124m, p=\u001B[39m\u001B[38;5;132;01m{p}\u001B[39;00m\u001B[38;5;124m, t=\u001B[39m\u001B[38;5;132;01m{t:.2e}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(x\u001B[38;5;241m=\u001B[39mx, p\u001B[38;5;241m=\u001B[39mp, t\u001B[38;5;241m=\u001B[39mt))\n\u001B[1;32m    102\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m t\n",
      "\u001B[0;31mValueError\u001B[0m: Line search did not converge at x=[ 4.55014146e-08 -1.22990350e-01], p=[-1.46540516e+04  4.90101639e-02], t=6.21e-12"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-09T13:42:55.407230Z",
     "start_time": "2025-01-09T13:42:55.357166Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from scipy.optimize import minimize, minimize_scalar\n",
    "from sklearn.datasets import make_regression\n",
    "\n",
    "x, y = make_regression(n_samples=100, n_features=10, noise=10, n_informative=3, random_state=42)\n",
    "\n",
    "\n",
    "def f(beta):\n",
    "    # enet criterion\n",
    "    y_hat = x @ beta\n",
    "    mse = np.mean((y - y_hat) ** 2)\n",
    "    penalty = 10.0 * np.sum(np.abs(beta)) + 5 * np.sum(beta ** 2)\n",
    "    return mse + penalty\n",
    "\n",
    "\n",
    "beta_0 = np.zeros(shape=(x.shape[1],))\n",
    "result = minimize(f, beta_0, method='BFGS')\n",
    "print(result)\n"
   ],
   "id": "4e0655aedee05ccf",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  message: Desired error not necessarily achieved due to precision loss.\n",
      "  success: False\n",
      "   status: 2\n",
      "      fun: 4095.9387337492435\n",
      "        x: [ 1.869e+00 -3.943e-05  2.888e-06  1.031e+01 -8.730e-05\n",
      "            -3.855e-06  3.466e-05  1.423e+00  6.406e-01 -5.678e-05]\n",
      "      nit: 44\n",
      "      jac: [-6.171e-02 -1.086e+01  1.628e+01 -2.887e-02 -8.228e+00\n",
      "            -1.065e+01  2.563e+00  1.318e-02  6.421e-02 -3.080e-01]\n",
      " hess_inv: [[ 5.202e-02 -2.913e-05 ... -1.341e-03 -8.904e-03]\n",
      "            [-2.913e-05  7.300e-07 ...  3.534e-06 -9.934e-05]\n",
      "            ...\n",
      "            [-1.341e-03  3.534e-06 ...  3.544e-02  6.536e-03]\n",
      "            [-8.904e-03 -9.934e-05 ...  6.536e-03  1.052e-01]]\n",
      "     nfev: 2088\n",
      "     njev: 189\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-09T14:07:53.564238Z",
     "start_time": "2025-01-09T14:07:53.078997Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# GD with line search Enet\n",
    "from scipy.optimize import approx_fprime, minimize_scalar\n",
    "\n",
    "\n",
    "def f_enet(beta, l1: float, l2: float) -> float:\n",
    "    # enet criterion\n",
    "    y_hat = x @ beta\n",
    "    mse = np.mean((y - y_hat) ** 2)\n",
    "    penalty = l1 * np.sum(np.abs(beta)) + l2 * np.sum(np.square(beta))\n",
    "    return mse + penalty\n",
    "\n",
    "\n",
    "def f_grad_enet(beta, l1: float, l2: float) -> np.ndarray:\n",
    "    grad_mse = x.T @ (x @ beta - y)\n",
    "    grad_penalty = l1 * np.sign(beta) + l2 * beta\n",
    "    return (grad_mse + grad_penalty) / x.shape[0]\n",
    "\n",
    "\n",
    "def gd_with_line_search(l1: float, l2: float, max_iter=1_000, loss_rtol: float = 1e-8) -> np.ndarray:\n",
    "    n, p = x.shape\n",
    "    beta = np.zeros(p, dtype=np.float64)\n",
    "    loss = f_enet(beta, l1, l2)\n",
    "    mom = 0.9\n",
    "    g = np.zeros_like(beta)\n",
    "\n",
    "    for i in range(max_iter):\n",
    "        grad = f_grad_enet(beta, l1, l2)\n",
    "        g = (1 - mom) * grad + mom * g\n",
    "        # grad = grad / np.linalg.norm(grad)\n",
    "        p = -g\n",
    "\n",
    "        t = 1.0\n",
    "\n",
    "        new_beta = beta + t * p\n",
    "        new_loss = f_enet(new_beta, l1, l2)\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            print(f\"Iteration {i:,}, loss {new_loss:.2e}, t {t:.2e}\")\n",
    "\n",
    "        beta = new_beta\n",
    "        loss = new_loss\n",
    "\n",
    "    return beta\n",
    "\n",
    "# [0.562, -0.0, 0.0, 5.217, -0.0, -0.0, 0.0, 0.368, 0.0, -0.0]\n",
    "b = gd_with_line_search(10.0, 10.0)\n",
    "print([round(float(b_), 3) for b_ in b])\n"
   ],
   "id": "3a501bf6c278a63b",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/25/l9j270ds30s7j_dj15xfs5t80000gn/T/ipykernel_32406/1153051812.py:35: RuntimeWarning: Method 'bounded' does not support relative tolerance in x; defaulting to absolute tolerance.\n",
      "  t = minimize_scalar(backtracking_line_search, bounds=(0, 1), tol=loss_rtol).x\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, loss 4.42e+03, t 7.88e-01\n",
      "Iteration 100, loss 4.42e+03, t 3.74e-09\n",
      "Iteration 200, loss 4.42e+03, t 3.74e-09\n",
      "Iteration 300, loss 4.42e+03, t 3.74e-09\n",
      "Iteration 400, loss 4.42e+03, t 3.74e-09\n",
      "Iteration 500, loss 4.42e+03, t 3.74e-09\n",
      "Iteration 600, loss 4.42e+03, t 3.74e-09\n",
      "Iteration 700, loss 4.42e+03, t 3.74e-09\n",
      "Iteration 800, loss 4.42e+03, t 3.74e-09\n",
      "Iteration 900, loss 4.42e+03, t 3.74e-09\n",
      "[1.293, 0.031, -0.306, 5.357, -0.122, 0.033, 0.375, 1.149, 0.835, -0.443]\n"
     ]
    }
   ],
   "execution_count": 60
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-09T14:05:08.309035Z",
     "start_time": "2025-01-09T14:05:05.531418Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.linear_model import ElasticNet\n",
    "\n",
    "\n",
    "def saga_enet(l1: float, l2: float, tol: float = 1e-6, max_iter: int = 50_000) -> np.ndarray:\n",
    "    n, p = x.shape\n",
    "    grad = np.zeros_like(x)\n",
    "    g = np.zeros(x.shape[1])\n",
    "    beta = np.zeros(x.shape[1])\n",
    "    lr = 1e-1\n",
    "    lr_target = 1e-10\n",
    "    lr_factor = (lr_target / lr) ** (1 / max_iter)\n",
    "    n_print = max_iter // 10\n",
    "\n",
    "    for i in range(max_iter):\n",
    "        idx = np.random.randint(n)\n",
    "        prev = grad[idx, :]\n",
    "        g -= prev\n",
    "        x_i = x[idx, :]\n",
    "        y_i = y[idx]\n",
    "        grad_i = (\n",
    "                     # MSE grad\n",
    "                         (x_i @ beta - y_i) * x_i\n",
    "                         # L1 grad\n",
    "                         + l1 * np.sign(beta)\n",
    "                         # L2 grad\n",
    "                         + l2 * beta\n",
    "                 ) / n\n",
    "        grad[idx, :] = grad_i\n",
    "        g += grad_i\n",
    "        beta -= g * lr\n",
    "\n",
    "        if np.linalg.norm(g) < tol:\n",
    "            print(f\"Converged at iteration {i:,}\")\n",
    "            break\n",
    "\n",
    "        if i % n_print == 0:\n",
    "            print(f\"Iteration {i:,} grad norm: {np.linalg.norm(g):.2e}, lr: {lr:.2e}\")\n",
    "        lr *= lr_factor\n",
    "\n",
    "    return beta\n",
    "\n",
    "\n",
    "for l1, l2 in product([0.1, 1.0, 10.0], [0.1, 1.0, 10.0]):\n",
    "    beta_saga = saga_enet(l1, l2)\n",
    "    beta_cd = ElasticNet(alpha=l1 + l2, l1_ratio=l1 / (l1 + l2), fit_intercept=False).fit(x, y).coef_\n",
    "    beta_gd = gd_with_line_search(l1, l2)\n",
    "\n",
    "    print(f\"\\nl1={l1}, l2={l2}, diff={np.linalg.norm(beta_saga - beta_cd) / np.linalg.norm(beta_cd):.3%}\")\n",
    "    for b, name in zip([beta_saga, beta_cd, beta_gd], [\"SAGA\", \"CD\", \"GD + LS\"]):\n",
    "        print(f\"{name: >10}: {[round(float(b_), 3) for b_ in b]}\")\n",
    "\n",
    "# beta = saga_enet()\n",
    "# print([round(float(b), 3) for b in beta])\n"
   ],
   "id": "5ecc1ef72c5f7337",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0 grad norm: 1.20e+00, lr: 1.00e-01\n",
      "Iteration 5,000 grad norm: 3.29e-05, lr: 1.26e-02\n",
      "Converged at iteration 5,813\n",
      "Iteration 0, loss 6.50e+02, t 7.89e-01\n",
      "Converged at iteration 2, loss 5.15e+02, t 3.74e-09\n",
      "\n",
      "l1=0.1, l2=0.1, diff=0.003%\n",
      "      SAGA: [15.193, 0.399, 0.164, 57.463, 1.273, 0.205, -0.313, 8.553, 1.007, -1.499]\n",
      "        CD: [15.193, 0.4, 0.163, 57.463, 1.273, 0.206, -0.314, 8.552, 1.007, -1.499]\n",
      "   GD + LS: [16.328, 0.129, 0.222, 59.29, 1.432, 1.15, 0.753, 8.908, 2.316, -2.705]\n",
      "Iteration 0 grad norm: 1.45e+00, lr: 1.00e-01\n",
      "Iteration 5,000 grad norm: 1.85e-05, lr: 1.26e-02\n",
      "Converged at iteration 5,797\n",
      "Iteration 0, loss 2.37e+03, t 4.61e-01\n",
      "Converged at iteration 5, loss 2.36e+03, t 3.36e-09\n",
      "\n",
      "l1=0.1, l2=1.0, diff=0.001%\n",
      "      SAGA: [8.356, 0.122, -0.56, 32.263, 0.15, 0.264, 0.839, 5.6, 2.438, -1.725]\n",
      "        CD: [8.356, 0.121, -0.56, 32.263, 0.15, 0.264, 0.839, 5.6, 2.438, -1.725]\n",
      "   GD + LS: [7.78, 0.183, -1.76, 32.058, -0.676, 0.211, 2.179, 6.798, 4.859, -2.609]\n",
      "Iteration 0 grad norm: 1.35e+00, lr: 1.00e-01\n",
      "Iteration 5,000 grad norm: 3.69e-06, lr: 1.26e-02\n",
      "Converged at iteration 5,313\n",
      "Iteration 0, loss 4.31e+03, t 8.95e-02\n",
      "Converged at iteration 3, loss 4.31e+03, t 3.74e-09\n",
      "\n",
      "l1=0.1, l2=10.0, diff=0.000%\n",
      "      SAGA: [1.487, 0.024, -0.291, 6.081, -0.093, 0.037, 0.371, 1.247, 0.847, -0.465]\n",
      "        CD: [1.487, 0.024, -0.291, 6.081, -0.093, 0.037, 0.371, 1.247, 0.847, -0.465]\n",
      "   GD + LS: [1.462, 0.036, -0.346, 6.058, -0.138, 0.037, 0.425, 1.299, 0.945, -0.501]\n",
      "Iteration 0 grad norm: 2.20e+00, lr: 1.00e-01\n",
      "Iteration 5,000 grad norm: 2.17e-01, lr: 1.26e-02\n",
      "Iteration 10,000 grad norm: 1.04e-01, lr: 1.58e-03\n",
      "Iteration 15,000 grad norm: 2.45e-01, lr: 2.00e-04\n",
      "Iteration 20,000 grad norm: 1.23e-01, lr: 2.51e-05\n",
      "Iteration 25,000 grad norm: 1.43e-01, lr: 3.16e-06\n",
      "Iteration 30,000 grad norm: 2.21e-01, lr: 3.98e-07\n",
      "Iteration 35,000 grad norm: 1.61e-01, lr: 5.01e-08\n",
      "Iteration 40,000 grad norm: 8.12e-02, lr: 6.31e-09\n",
      "Iteration 45,000 grad norm: 1.63e-01, lr: 7.94e-10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/25/l9j270ds30s7j_dj15xfs5t80000gn/T/ipykernel_32406/3626109192.py:32: RuntimeWarning: Method 'bounded' does not support relative tolerance in x; defaulting to absolute tolerance.\n",
      "  t = minimize_scalar(backtracking_line_search, bounds=(0, 1), tol=loss_rtol).x\n",
      "/var/folders/25/l9j270ds30s7j_dj15xfs5t80000gn/T/ipykernel_32406/3626109192.py:32: RuntimeWarning: Method 'bounded' does not support relative tolerance in x; defaulting to absolute tolerance.\n",
      "  t = minimize_scalar(backtracking_line_search, bounds=(0, 1), tol=loss_rtol).x\n",
      "/var/folders/25/l9j270ds30s7j_dj15xfs5t80000gn/T/ipykernel_32406/3626109192.py:32: RuntimeWarning: Method 'bounded' does not support relative tolerance in x; defaulting to absolute tolerance.\n",
      "  t = minimize_scalar(backtracking_line_search, bounds=(0, 1), tol=loss_rtol).x\n",
      "/var/folders/25/l9j270ds30s7j_dj15xfs5t80000gn/T/ipykernel_32406/3626109192.py:32: RuntimeWarning: Method 'bounded' does not support relative tolerance in x; defaulting to absolute tolerance.\n",
      "  t = minimize_scalar(backtracking_line_search, bounds=(0, 1), tol=loss_rtol).x\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, loss 7.39e+02, t 7.80e-01\n",
      "Converged at iteration 2, loss 5.99e+02, t 3.74e-09\n",
      "\n",
      "l1=1.0, l2=0.1, diff=0.000%\n",
      "      SAGA: [14.224, 0.0, 0.0, 56.846, 0.394, -0.0, 0.0, 7.917, 0.237, -0.499]\n",
      "        CD: [14.224, 0.0, -0.0, 56.846, 0.395, -0.0, -0.0, 7.917, 0.237, -0.499]\n",
      "   GD + LS: [16.229, 0.127, 0.149, 59.106, 1.374, 1.122, 0.818, 8.958, 2.453, -2.738]\n",
      "Iteration 0 grad norm: 2.71e+00, lr: 1.00e-01\n",
      "Iteration 5,000 grad norm: 1.16e-01, lr: 1.26e-02\n",
      "Iteration 10,000 grad norm: 6.06e-02, lr: 1.58e-03\n",
      "Iteration 15,000 grad norm: 1.32e-01, lr: 2.00e-04\n",
      "Iteration 20,000 grad norm: 1.64e-01, lr: 2.51e-05\n",
      "Iteration 25,000 grad norm: 9.81e-02, lr: 3.16e-06\n",
      "Iteration 30,000 grad norm: 1.15e-01, lr: 3.98e-07\n",
      "Iteration 35,000 grad norm: 1.43e-01, lr: 5.01e-08\n",
      "Iteration 40,000 grad norm: 1.20e-01, lr: 6.31e-09\n",
      "Iteration 45,000 grad norm: 2.04e-01, lr: 7.94e-10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/25/l9j270ds30s7j_dj15xfs5t80000gn/T/ipykernel_32406/3626109192.py:32: RuntimeWarning: Method 'bounded' does not support relative tolerance in x; defaulting to absolute tolerance.\n",
      "  t = minimize_scalar(backtracking_line_search, bounds=(0, 1), tol=loss_rtol).x\n",
      "/var/folders/25/l9j270ds30s7j_dj15xfs5t80000gn/T/ipykernel_32406/3626109192.py:32: RuntimeWarning: Method 'bounded' does not support relative tolerance in x; defaulting to absolute tolerance.\n",
      "  t = minimize_scalar(backtracking_line_search, bounds=(0, 1), tol=loss_rtol).x\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, loss 2.42e+03, t 4.56e-01\n",
      "Converged at iteration 6, loss 2.42e+03, t 6.25e-09\n",
      "\n",
      "l1=1.0, l2=1.0, diff=0.001%\n",
      "      SAGA: [7.827, 0.0, -0.165, 31.934, 0.0, 0.0, 0.402, 5.194, 2.058, -1.235]\n",
      "        CD: [7.827, 0.0, -0.165, 31.934, 0.0, 0.0, 0.403, 5.194, 2.058, -1.235]\n",
      "   GD + LS: [7.713, 0.181, -1.74, 31.777, -0.667, 0.21, 2.156, 6.734, 4.809, -2.583]\n",
      "Iteration 0 grad norm: 2.03e+00, lr: 1.00e-01\n",
      "Iteration 5,000 grad norm: 5.25e-02, lr: 1.26e-02\n",
      "Iteration 10,000 grad norm: 7.81e-02, lr: 1.58e-03\n",
      "Iteration 15,000 grad norm: 1.25e-01, lr: 2.00e-04\n",
      "Iteration 20,000 grad norm: 4.84e-02, lr: 2.51e-05\n",
      "Iteration 25,000 grad norm: 6.16e-02, lr: 3.16e-06\n",
      "Iteration 30,000 grad norm: 1.32e-01, lr: 3.98e-07\n",
      "Iteration 35,000 grad norm: 1.42e-01, lr: 5.01e-08\n",
      "Iteration 40,000 grad norm: 4.85e-02, lr: 6.31e-09\n",
      "Iteration 45,000 grad norm: 9.58e-02, lr: 7.94e-10\n",
      "Iteration 0, loss 4.32e+03, t 8.85e-02\n",
      "Converged at iteration 3, loss 4.32e+03, t 3.74e-09\n",
      "\n",
      "l1=1.0, l2=10.0, diff=0.000%\n",
      "      SAGA: [1.402, 0.0, -0.211, 6.004, -0.011, 0.0, 0.291, 1.168, 0.769, -0.382]\n",
      "        CD: [1.402, 0.0, -0.211, 6.004, -0.011, 0.0, 0.291, 1.168, 0.769, -0.382]\n",
      "   GD + LS: [1.447, 0.035, -0.343, 5.994, -0.137, 0.037, 0.42, 1.285, 0.935, -0.496]\n",
      "Iteration 0 grad norm: 8.42e-01, lr: 1.00e-01\n",
      "Iteration 5,000 grad norm: 1.03e+00, lr: 1.26e-02\n",
      "Iteration 10,000 grad norm: 1.69e+00, lr: 1.58e-03\n",
      "Iteration 15,000 grad norm: 1.69e+00, lr: 2.00e-04\n",
      "Iteration 20,000 grad norm: 1.04e+00, lr: 2.51e-05\n",
      "Iteration 25,000 grad norm: 1.80e+00, lr: 3.16e-06\n",
      "Iteration 30,000 grad norm: 2.01e+00, lr: 3.98e-07\n",
      "Iteration 35,000 grad norm: 1.30e+00, lr: 5.01e-08\n",
      "Iteration 40,000 grad norm: 2.18e+00, lr: 6.31e-09\n",
      "Iteration 45,000 grad norm: 1.86e+00, lr: 7.94e-10\n",
      "Iteration 0, loss 1.57e+03, t 6.95e-01\n",
      "Converged at iteration 14, loss 1.42e+03, t 7.89e-09\n",
      "\n",
      "l1=10.0, l2=0.1, diff=0.001%\n",
      "      SAGA: [3.894, 0.0, -0.0, 50.159, -0.0, -0.0, 0.0, 0.209, 0.0, -0.0]\n",
      "        CD: [3.894, 0.0, -0.0, 50.158, 0.0, -0.0, -0.0, 0.209, 0.0, -0.0]\n",
      "   GD + LS: [15.565, 0.134, -0.194, 57.955, 1.028, 0.764, 0.853, 9.22, 2.922, -2.746]\n",
      "Iteration 0 grad norm: 3.10e-01, lr: 1.00e-01\n",
      "Iteration 5,000 grad norm: 1.82e+00, lr: 1.26e-02\n",
      "Iteration 10,000 grad norm: 1.89e+00, lr: 1.58e-03\n",
      "Iteration 15,000 grad norm: 1.86e+00, lr: 2.00e-04\n",
      "Iteration 20,000 grad norm: 2.47e+00, lr: 2.51e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/25/l9j270ds30s7j_dj15xfs5t80000gn/T/ipykernel_32406/3626109192.py:32: RuntimeWarning: Method 'bounded' does not support relative tolerance in x; defaulting to absolute tolerance.\n",
      "  t = minimize_scalar(backtracking_line_search, bounds=(0, 1), tol=loss_rtol).x\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 25,000 grad norm: 1.79e+00, lr: 3.16e-06\n",
      "Iteration 30,000 grad norm: 1.72e+00, lr: 3.98e-07\n",
      "Iteration 35,000 grad norm: 1.86e+00, lr: 5.01e-08\n",
      "Iteration 40,000 grad norm: 1.41e+00, lr: 6.31e-09\n",
      "Iteration 45,000 grad norm: 2.10e+00, lr: 7.94e-10\n",
      "Iteration 0, loss 2.91e+03, t 4.06e-01\n",
      "Converged at iteration 6, loss 2.90e+03, t 8.02e-09\n",
      "\n",
      "l1=10.0, l2=1.0, diff=0.001%\n",
      "      SAGA: [2.683, 0.0, 0.0, 28.083, 0.0, -0.0, -0.0, 1.038, 0.0, 0.0]\n",
      "        CD: [2.683, 0.0, -0.0, 28.084, 0.0, -0.0, 0.0, 1.038, 0.0, -0.0]\n",
      "   GD + LS: [7.016, 0.16, -1.555, 28.862, -0.585, 0.192, 1.934, 6.09, 4.322, -2.329]\n",
      "Iteration 0 grad norm: 6.04e-01, lr: 1.00e-01\n",
      "Iteration 5,000 grad norm: 5.90e-01, lr: 1.26e-02\n",
      "Iteration 10,000 grad norm: 1.16e+00, lr: 1.58e-03\n",
      "Iteration 15,000 grad norm: 1.68e+00, lr: 2.00e-04\n",
      "Iteration 20,000 grad norm: 1.72e+00, lr: 2.51e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/25/l9j270ds30s7j_dj15xfs5t80000gn/T/ipykernel_32406/3626109192.py:32: RuntimeWarning: Method 'bounded' does not support relative tolerance in x; defaulting to absolute tolerance.\n",
      "  t = minimize_scalar(backtracking_line_search, bounds=(0, 1), tol=loss_rtol).x\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 25,000 grad norm: 1.40e+00, lr: 3.16e-06\n",
      "Iteration 30,000 grad norm: 1.33e+00, lr: 3.98e-07\n",
      "Iteration 35,000 grad norm: 2.45e+00, lr: 5.01e-08\n",
      "Iteration 40,000 grad norm: 1.91e+00, lr: 6.31e-09\n",
      "Iteration 45,000 grad norm: 1.70e+00, lr: 7.94e-10\n",
      "Iteration 0, loss 4.42e+03, t 7.88e-02\n",
      "Converged at iteration 3, loss 4.42e+03, t 7.07e-09\n",
      "\n",
      "l1=10.0, l2=10.0, diff=0.000%\n",
      "      SAGA: [0.562, -0.0, 0.0, 5.217, -0.0, -0.0, 0.0, 0.368, 0.0, -0.0]\n",
      "        CD: [0.562, 0.0, -0.0, 5.217, -0.0, 0.0, 0.0, 0.368, 0.0, -0.0]\n",
      "   GD + LS: [1.293, 0.031, -0.306, 5.357, -0.122, 0.033, 0.375, 1.148, 0.835, -0.443]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/25/l9j270ds30s7j_dj15xfs5t80000gn/T/ipykernel_32406/3626109192.py:32: RuntimeWarning: Method 'bounded' does not support relative tolerance in x; defaulting to absolute tolerance.\n",
      "  t = minimize_scalar(backtracking_line_search, bounds=(0, 1), tol=loss_rtol).x\n"
     ]
    }
   ],
   "execution_count": 57
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-07T12:01:11.010855Z",
     "start_time": "2025-01-07T12:01:10.990321Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def adam_enet(beta_1: float = 0.98, beta_2: float = 0.99, max_iter: int = 200_000) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Minimize the Elastic Net criterion using the Adam optimizer.\n",
    "\n",
    "    Args:\n",
    "      beta_1: Exponential decay rate for the first moment estimates.\n",
    "      beta_2: Exponential decay rate for the second moment estimates.\n",
    "      max_iter: Maximum number of iterations.\n",
    "\n",
    "    Returns:\n",
    "      An array of the estimated coefficients.\n",
    "    \"\"\"\n",
    "    n, p = x.shape\n",
    "    beta = np.zeros(p)\n",
    "    m = np.zeros(p)\n",
    "    v = np.ones(p)\n",
    "    epsilon = 1e-8\n",
    "    alpha = 0.001  # Learning rate\n",
    "\n",
    "    for t in range(1, max_iter + 1):\n",
    "        batch_idx = np.random.randint(n, size=32)\n",
    "        batch = x[batch_idx, :]\n",
    "        grad = f_grad_enet(batch, beta) / n\n",
    "        m = beta_1 * m + (1 - beta_1) * grad\n",
    "        v = beta_2 * v + (1 - beta_2) * np.square(grad)\n",
    "        m_hat = m / (1 - beta_1 ** t)\n",
    "        v_hat = v / (1 - beta_2 ** t)\n",
    "        beta -= alpha * m_hat / (np.sqrt(v_hat) + epsilon)\n",
    "\n",
    "        if t % (max_iter // 10) == 0:\n",
    "            print(f\"Iteration {t:,} grad norm: {np.linalg.norm(grad)}\")\n",
    "\n",
    "    return beta\n",
    "\n",
    "\n",
    "beta = adam_enet()\n",
    "print(np.round(beta, 3))"
   ],
   "id": "3a2369632b232c7f",
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (100,) (32,) ",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[118], line 35\u001B[0m\n\u001B[1;32m     31\u001B[0m             \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mIteration \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mt\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m,\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m grad norm: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mnp\u001B[38;5;241m.\u001B[39mlinalg\u001B[38;5;241m.\u001B[39mnorm(grad)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     33\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m beta\n\u001B[0;32m---> 35\u001B[0m beta \u001B[38;5;241m=\u001B[39m \u001B[43madam_enet\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     36\u001B[0m \u001B[38;5;28mprint\u001B[39m(np\u001B[38;5;241m.\u001B[39mround(beta, \u001B[38;5;241m3\u001B[39m))\n",
      "Cell \u001B[0;32mIn[118], line 23\u001B[0m, in \u001B[0;36madam_enet\u001B[0;34m(beta_1, beta_2, max_iter)\u001B[0m\n\u001B[1;32m     21\u001B[0m batch_idx \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mrandom\u001B[38;5;241m.\u001B[39mrandint(n, size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m32\u001B[39m)\n\u001B[1;32m     22\u001B[0m batch \u001B[38;5;241m=\u001B[39m x[batch_idx, :]\n\u001B[0;32m---> 23\u001B[0m grad \u001B[38;5;241m=\u001B[39m \u001B[43mf_grad_enet\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbeta\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;241m/\u001B[39m n\n\u001B[1;32m     24\u001B[0m m \u001B[38;5;241m=\u001B[39m beta_1 \u001B[38;5;241m*\u001B[39m m \u001B[38;5;241m+\u001B[39m (\u001B[38;5;241m1\u001B[39m \u001B[38;5;241m-\u001B[39m beta_1) \u001B[38;5;241m*\u001B[39m grad\n\u001B[1;32m     25\u001B[0m v \u001B[38;5;241m=\u001B[39m beta_2 \u001B[38;5;241m*\u001B[39m v \u001B[38;5;241m+\u001B[39m (\u001B[38;5;241m1\u001B[39m \u001B[38;5;241m-\u001B[39m beta_2) \u001B[38;5;241m*\u001B[39m np\u001B[38;5;241m.\u001B[39msquare(grad)\n",
      "Cell \u001B[0;32mIn[108], line 18\u001B[0m, in \u001B[0;36mf_grad_enet\u001B[0;34m(x, beta, dx)\u001B[0m\n\u001B[1;32m     16\u001B[0m     beta_minus \u001B[38;5;241m=\u001B[39m beta\u001B[38;5;241m.\u001B[39mcopy()\n\u001B[1;32m     17\u001B[0m     beta_minus[i] \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m=\u001B[39m dx\n\u001B[0;32m---> 18\u001B[0m     grad[i] \u001B[38;5;241m=\u001B[39m (\u001B[43mf_enet\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbeta_plus\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;241m-\u001B[39m f_enet(x, beta_minus)) \u001B[38;5;241m/\u001B[39m (\u001B[38;5;241m2\u001B[39m \u001B[38;5;241m*\u001B[39m dx)\n\u001B[1;32m     19\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m grad\n",
      "Cell \u001B[0;32mIn[108], line 6\u001B[0m, in \u001B[0;36mf_enet\u001B[0;34m(x, beta)\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mf_enet\u001B[39m(x, beta):\n\u001B[1;32m      4\u001B[0m     \u001B[38;5;66;03m# enet criterion\u001B[39;00m\n\u001B[1;32m      5\u001B[0m     y_hat \u001B[38;5;241m=\u001B[39m x \u001B[38;5;241m@\u001B[39m beta\n\u001B[0;32m----> 6\u001B[0m     mse \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mmean((\u001B[43my\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m-\u001B[39;49m\u001B[43m \u001B[49m\u001B[43my_hat\u001B[49m)\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m2\u001B[39m)\n\u001B[1;32m      7\u001B[0m     penalty \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m10.0\u001B[39m \u001B[38;5;241m*\u001B[39m np\u001B[38;5;241m.\u001B[39msum(np\u001B[38;5;241m.\u001B[39mabs(beta)) \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m5\u001B[39m \u001B[38;5;241m*\u001B[39m np\u001B[38;5;241m.\u001B[39msum(beta\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m2\u001B[39m)\n\u001B[1;32m      8\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m mse \u001B[38;5;241m+\u001B[39m penalty\n",
      "\u001B[0;31mValueError\u001B[0m: operands could not be broadcast together with shapes (100,) (32,) "
     ]
    }
   ],
   "execution_count": 118
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
