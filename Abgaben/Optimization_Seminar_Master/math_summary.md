The abstract reads:

We investigate the behavior of quasi-Newton algorithms applied to minimize a nonsmooth function $f$, not necessarily convex. We introduce an inexact line search that generates a sequence of nested intervals containing a set of points of nonzero measure that satisfy the Armijo and Wolfe conditions if $f$ is absolutely continuous along the line. Furthermore, the line search is guaranteed to terminate if $f$ is semi-algebraic. It seems quite difficult to establish a convergence theorem for quasi-Newton methods applied to such general classes of functions, so we give a careful analysis of a special but illuminating case, the Euclidean norm, in one variable using the inexact line search and in two variables assuming that the line search is exact. In practice, we find that when $f$ is locally Lipschitz and semi-algebraic with bounded sublevel sets, the BFGS (Broyden-Fletcher-Goldfarb-Shanno) method with the inexact line search almost always generates sequences whose cluster points are Clarke stationary and with function values converging R-linearly to a Clarke stationary value. We give references documenting the successful use of BFGS in a variety of nonsmooth applications, particularly the design of low-order controllers for linear dynamical systems. We conclude with a challenging open question. [cite: 1, 2, 3, 4, 5]

### Specific Statements and their Explanation

*   **quasi-Newton algorithms**: These are a family of optimization algorithms that use an approximation of the Hessian matrix (second derivatives) to find the minimum of a function. [cite: 1, 2, 3, 4, 5]
*   **nonsmooth function**: A function that is not differentiable at one or more points. This means it has "kinks" or "corners" where the usual derivative doesn't exist. [cite: 1, 2, 3, 4, 5]
*   **inexact line search**: A technique used in optimization to find an approximate step size that reduces the function value sufficiently along a given search direction. The Armijo and Wolfe conditions are standard criteria used to determine an acceptable step size in smooth optimization. [cite: 1, 2, 3, 4, 5]
*   **Armijo and Wolfe conditions**: The Armijo condition ensures that the step size decreases the function value sufficiently. The Wolfe condition ensures that the step size is not too small. [cite: 1, 2, 3, 4, 5]
*   **absolutely continuous along the line**: A function is absolutely continuous if it can be reconstructed from its derivative, implying a certain degree of smoothness. [cite: 1, 2, 3, 4, 5]
*   **semi-algebraic**: A function is semi-algebraic if its graph can be described by a finite number of polynomial equations and inequalities. Semi-algebraic functions have nice properties that make them amenable to analysis in optimization. [cite: 1, 2, 3, 4, 5]
*   **Euclidean norm**: The Euclidean norm is the distance from the origin to a point in a multi-dimensional space. It's the most common way to measure distances. [cite: 1, 2, 3, 4, 5]
*   **locally Lipschitz**: A function is locally Lipschitz if, around every point, there's a neighborhood where the function's rate of change is bounded. [cite: 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20]
*   **bounded sublevel sets**: A sublevel set is the set of points where the function value is less than or equal to a given constant. Bounded sublevel sets mean these sets don't "stretch off to infinity." [cite: 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20]
*   **BFGS**: The Broyden-Fletcher-Goldfarb-Shanno (BFGS) method is a popular quasi-Newton algorithm. [cite: 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20]
*   **Clarke stationary**: A point is Clarke stationary if it satisfies a necessary optimality condition for nonsmooth functions. [cite: 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20]
*   **R-linearly**: A sequence converges R-linearly if its distance to the limit point decreases at a rate similar to a geometric sequence. [cite: 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20]

### Contribution of the Paper

This paper investigates how quasi-Newton methods perform when used to minimize nonsmooth functions, which is a challenging problem in optimization. The authors introduce a new type of inexact line search suitable for nonsmooth functions and provide a detailed analysis of its properties. They also give a careful analysis of the BFGS method on the Euclidean norm function as a special case. Through numerical experiments, they demonstrate that BFGS with their inexact line search is effective in finding Clarke stationary points for a wide variety of nonsmooth functions. [cite: 1, 2, 3, 4, 5]