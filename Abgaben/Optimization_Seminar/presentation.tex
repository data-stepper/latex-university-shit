\documentclass[compress]{beamer}

% Information to be included in the title page:

\title{
	Automatic Differentiation:
	An Overview of methods used and presentation of one
	specific structure-exploiting Algorithm that achieves
	almost optimal complexity for Kantorovic trees.
}

\author{Bent MÃ¼ller}
\institute{University of Hamburg}
\date{17.05.2022}

% Standard beamer class setup, configure as needed

\usepackage{tikz}
\usetikzlibrary{automata, positioning}

\setbeamertemplate{headline}[default]
\setbeamertemplate{navigation symbols}{}
\mode<beamer>{\setbeamertemplate{blocks}[rounded][shadow=true]}
\setbeamercovered{transparent}
\setbeamercolor{block body example}{fg=blue, bg=black!20}

\useoutertheme[subsection=true]{miniframes}
\usetheme{Bergen}

\def\Z{{\mathbb Z}}
\def\N{{\mathbb N}}
\def\Q{{\mathbb Q}}
\def\R{{\mathbb R}}
\def\C{{\mathbb C}}
\def\S{{\mathbb S}}
\def\K{{\mathbb K}}
\def\T{{\mathbb T}}

\def\cA{{\mathcal A}}
\def\cF{{\mathcal F}}
\def\cG{{\mathcal G}}
\def\cM{{\mathcal M}}
\def\cN{{\mathcal N}}
\def\cP{{\mathcal P}}
\def\cS{{\mathcal S}}

\begin{document}

\begin{frame}
	\titlepage
\end{frame}

\section{Overview}
\subsection{Table of Contents}
\begin{frame}
	% Table of contents
	\tableofcontents
\end{frame}

\subsection{General problem layout}

\begin{frame}
	\frametitle{What is automatic differentiation?}

	Given a function $f: \R^{n} \longrightarrow \R$ that is
	\textit{twice differentiable}, we want to
	\textit{efficiently}
	and with good \textit{numerical stability} compute
	\vspace{5mm}

	\begin{itemize}
		\item $f(x)$, the value of our function at a point $x$ in $\R^{n}$,
		\item $\nabla f(x)$, the gradient of our function at $x$, and
		\item $\nabla^2 f(x)$, the Hessian of our function at $x$.
	\end{itemize}
\end{frame}

% Make this frame aligned to the top
\begin{frame}[t]
	\frametitle{Quick reminder of the symbols}

	\only<1->{
		\vspace{5mm}
		Since $f: \R^{n} \longrightarrow \R$, we know that
		$f(x) \in \R$, now we define the Gradient and Hessian
		for $x \in D \subset \R^n$ where $D$ is an
		open subset in $\R^n$ as follows:
	}

	\only<2>{
		\vspace{5mm}
		\[
			\nabla f(x) := \begin{pmatrix}
				\frac{ \partial f(x) }{ \partial x_1 } \\
				\frac{ \partial f(x) }{ \partial x_2 } \\
				\vdots                                 \\
				\frac{ \partial f(x) }{ \partial x_n }
			\end{pmatrix}
		\]

		\vspace{5mm}

		The Gradient is the n-dimensional
		\textit{vector} of partial derivatives of $f$ at $x$.
		It describes how $f$ changes with respect to each of the
		$n$ variables $x_1, \ldots, x_n$.
	}

	\only<3->{
		\[
			\nabla^2 f(x) := \begin{pmatrix}
				\frac{ \partial ^2 f(x) }{ \partial x_1 ^2 }           &
				\frac{ \partial ^2 f(x) }{ \partial x_1 \partial x_2 } &
				\cdots                                                 &
				\frac{ \partial ^2 f(x) }{ \partial x_1 \partial x_n }   \\
				\frac{ \partial ^2 f(x) }{ \partial x_2 \partial x_1 } &
				\frac{ \partial ^2 f(x) }{ \partial x_2 ^2 }           &
				\cdots                                                 &
				\frac{ \partial ^2 f(x) }{ \partial x_2 \partial x_n }   \\
				\vdots                                                 &
				\vdots                                                 &
				\ddots                                                 &
				\vdots                                                   \\
				\frac{ \partial ^2 f(x) }{ \partial x_n \partial x_1 } &
				\frac{ \partial ^2 f(x) }{ \partial x_n \partial x_2 } &
				\cdots                                                 &
				\frac{ \partial ^2 f(x) }{ \partial x_n ^2 }             \\
			\end{pmatrix}
		\]
	}

	\only<3>{
		The Hessian is the real matrix of all \textit{second order}
		partial derivatives of $f$ at $x$. It describes how the gradient
		changes with respect to each of the $n$ variables $x_1, \ldots, x_n$.
	}

	\only<4>{
		Each row of the Hessian can be regarded as the gradient w.r.t. each
		component of the gradient vector of $f$. Specifically, it is the
		\textit{Jacobian} matrix of the gradient.
	}

\end{frame}

\subsection{Characterizing Sequence}
\begin{frame}[t]
	\frametitle{ Characterizing Sequence }

	\vspace{5mm}

	\only<1>{
		We will only discuss functions that can be written as compositions of:
		\vspace{5mm}

		\begin{itemize}
			\item constant functions \mathcal{C},
			\item unary functions \mathcal{U} and
			\item binary functions \mathcal{B}.
		\end{itemize}

		\vspace{5mm}
		Where \mathcal{C, U} and \mathcal{B} are the corresponding
		function spaces.

		\vspace{5mm}
		The characterizing sequence computes $f(x)$ in $m$-steps, with each
		step depending \textit{only} on previously computed steps.
	}

	\only<2>{
		We decompose the computation of $f(x)$ as follows:

		% Char sequence for f(x)
		\begin{itemize}
			\item $f_i = x_i$ for $i \in \{
				      1, \ldots, n
				      \} $
			\item $f_{i + n} = \begin{cases}
					      \omega_i                    & \text{ if } \omega_i \in \mathcal{C} \\
					      \omega_i (f_{k_i})          & \text{ if } \omega_i \in \mathcal{U} \\
					      \omega_i (f_{k_i}, f_{l_i}) & \text{ if } \omega_i \in \mathcal{B}
				      \end{cases}$
			      \qquad for $i \in \{
				      1, \ldots, m
				      \}$
			\item $f_{m + n} = f(x)$
			\item $k_i, l_i < i + n$  \text{ and }
			\item $\{
				      n+1, \ldots, n+m - 1
				      \} \subset \bigcup_{i=1}^{m} \{
				      k_i, l_i
				      \}$
		\end{itemize}
		The last condition ensures that each of the $m$-steps in the computation
		is actually used to compute $f(x)$.
		\[
			\Rightarrow S := \left(
			(\omega_i, k_i, l_i)
			\right)_{i \in \{
					1, \ldots, m
					\} }
			\text{is char. seq. for f}
		\]
		that is defined in the following.
	}

	\only<3-4>{
		More specifically, we set $I := \{
			1, \ldots, m
			\} $ and $J := \{
			1, \ldots, n + m - 1
			\} $ as two sets of indices.

		\begin{align*}
			\Rightarrow S & := \left(
			(\omega_i, k_i, l_i)
			\right)_{i \in \{
					1, \ldots, m
			\} }                       \\
			              & \in \left(
			\left(
				\mathcal{C} \times \{
				0
				\}^2
				\right) \cup
			\left(
				\mathcal{U} \times J \times \{
				0
				\} \cup
				\left(
					\mathcal{B} \times J^2
					\right)
				\right) \right) ^m
		\end{align*}

		The above is equivalent to the following three cases:
	}

	\only<4>{
		\vspace{5mm}

		\begin{itemize}
			\item if $\omega_i \in \mathcal{C} \Rightarrow
				      k_i = l_i = 0$
			\item if $\omega_i \in \mathcal{U} \Rightarrow
				      k_i \in J, l_i = 0$
			\item if $\omega_i \in \mathcal{B} \Rightarrow
				      k_i, l_i \in J$
		\end{itemize}
	}
\end{frame}

\begin{frame}
	\frametitle{The Computational Graph}
	An example computational graph constructed by a characterizing sequence
	for $n=4$ and $m=4$.

	\begin{center}
		\begin{tikzpicture}
			% Add the states
			\node[state]			 (a) {$x_1$};
			\node[state, right=of a] (b) {$x_2$};
			\node[state, right=of b] (c) {$x_3$};
			\node[state, right=of c] (d) {$x_4$};
			\node[state, below=of a] (e) {$f_{1 + n}$};
			\node[state, below=of b] (f) {$f_{2 + n}$};
			\node[state, below=of c] (g) {$f_{3 + n}$};
			\node[state, below=of f] (h) {$f_{m + n} = f(x)$};

			% Connect the states with arrows
			\draw[every loop]
			(a) edge[bend right, auto=right] node {$\omega_1 (k_1)$} (e)
			(b) edge[bend right, auto=right] node {$\omega_2 (k_2, l_2)$} (f)
			(c) edge[bend right, auto=left] node {$\omega_2 (k_2, l_2)$} (f)
			(c) edge[bend left, auto=left] node {$\omega_3 (k_3, l_3)$} (g)
			(f) edge[bend right, auto=right] node {$\omega_3 (k_3, l_3)$} (g)
			(e) edge[bend right, auto=right] node {$\omega_4 (k_4, l_4)$} (h)
			(g) edge[bend left, auto=left] node {$\omega_4 (k_4, l_4)$} (h)

		\end{tikzpicture}
	\end{center}
\end{frame}

\section{Operation of Auto-Diff}
\subsection{Forward mode}
\begin{frame}[t]
	\frametitle{Computing the Gradient $\nabla f(x)$}

	\only<1>{
		The idea is to compute the gradient and hessian of $f$ stepwise using
		the characterizing sequence $S$.
	}

	\only<2->{
		\begin{itemize}
			\item $g_j = e_j$ with $(e_j)_k = 1_{k=j}$
			      and $e_j \in \R^n$ for $j \in \{
				      1, \ldots, n
				      \}$
			\item $g_{i+n} = \begin{cases}
					      0                                                & \text{ if } \omega_i \in \mathcal{C} \\
					      \omega_i ' (f_{k_i}) g_{k_i}                     & \text{ if } \omega_i \in \mathcal{U} \\
					      \partial_a \omega_i (f_{k_i}, f_{l_i}) g_{k_i}
					      + \partial_b \omega_i (f_{k_i}, f_{l_i}) g_{l_i} & \text{ if } \omega_i \in \mathcal{B}
				      \end{cases}$ ,
			      for $i \in \{
				      1, \ldots, m
				      \}$
		\end{itemize}
		Note the following:
		\vspace{5mm}
	}

	\only<3>{
		The $g_i$ are $n$-dimensional vectors $\iff g_i \in \R^n$
	}

	\only<4>{
		For $\omega_i \in \mathcal{U}$ we have:
		\begin{gather*}
			\omega_i ' (f_{k_i}) \in \R \text{ since }
			\omega_i : \R \to \R \\
			\Rightarrow \omega_i ' (f_{k_i}) g_{k_i} \in \R^n
		\end{gather*}
	}

	\only<5>{
		For $\omega_i \in \mathcal{B}$ we have:
		\begin{align*}
			\nabla \omega_i (f_{k_i}, f_{l_i})
			 & =
			\partial_a \omega_i (f_{k_i}, f_{l_i}) \nabla f_{k_i} +
			\partial_b \omega_i (f_{k_i}, f_{l_i}) \nabla f_{l_i} \\
			 & =
			\partial_a \omega_i (f_{k_i}, f_{l_i}) g_{k_i} +
			\partial_b \omega_i (f_{k_i}, f_{l_i}) g_{l_i}
		\end{align*}

		With $\partial_a \omega_i (f_{k_i}, f_{l_i})$ being the partial derivative
		in the \textit{first} argument, that is w.r.t. $f_{k_i}$.
	}

	\only<6>{
		Thus, we have for
		$i \in \{
			1, \ldots, n + m
			\} $:
		\[
			g_i = \nabla f_i = \begin{pmatrix}
				\frac{ \partial f_i }{ \partial x_1 } \\
				\frac{ \partial f_i }{ \partial x_2 } \\
				\vdots                                \\
				\frac{ \partial f_i }{ \partial x_n } \\
			\end{pmatrix}
		\]
	}

	\only<7>{
		So for all $i \in \{
			1, \ldots, n + m
			\} $, $g_i$ is really the gradient vector of $f_i$ w.r.t. all
		inputs $\left(
			x_1, \ldots, x_n
			\right) $.

		\vspace{5mm}
		$\Rightarrow$ Auto-Diff applies the chain rule iteratively.
	}
\end{frame}

\begin{frame}
	\frametitle{Computing the Hessian $\nabla^2 f(x)$}

\end{frame}


\subsection{Reverse mode}
\subsection{Algorithmic complexity}

\section{The Algorithm}
\subsection{Kantorovic trees}
\subsection{Mode of operation}
\subsection{Proof of complexity}

\begin{frame}
	World
\end{frame}

\section{Applications}
\subsection{Use in optimization Algorithms}

\begin{frame}
	World
\end{frame}

\section{Literature}
\begin{frame}
	Some references
\end{frame}

\end{document}
