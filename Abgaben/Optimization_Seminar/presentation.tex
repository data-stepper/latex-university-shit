\documentclass[compress]{beamer}

% Information to be included in the title page:

\title{
	Automatic Differentiation:
	An Overview of Forward and Reverse Mode
	in Applications to Optimization Problems
}

\author{Bent MÃ¼ller}
\institute{University of Hamburg}
\date{17.05.2022}

% Standard beamer class setup, configure as needed

\usepackage{tikz}
\usetikzlibrary{automata, positioning}

\setbeamertemplate{headline}[default]
\setbeamertemplate{navigation symbols}{}
\mode<beamer>{\setbeamertemplate{blocks}[rounded][shadow=true]}
\setbeamercovered{transparent}
\setbeamercolor{block body example}{fg=blue, bg=black!20}

\useoutertheme[subsection=true]{miniframes}
\usetheme{Bergen}

\def\Z{{\mathbb Z}}
\def\N{{\mathbb N}}
\def\Q{{\mathbb Q}}
\def\R{{\mathbb R}}
\def\C{{\mathbb C}}
\def\S{{\mathbb S}}
\def\K{{\mathbb K}}
\def\T{{\mathbb T}}

\def\cA{{\mathcal A}}
\def\cF{{\mathcal F}}
\def\cG{{\mathcal G}}
\def\cM{{\mathcal M}}
\def\cN{{\mathcal N}}
\def\cP{{\mathcal P}}
\def\cS{{\mathcal S}}

\begin{document}

\begin{frame}
	\titlepage
\end{frame}

\section{Overview}
\subsection{Table of Contents}
\begin{frame}
	% Table of contents
	\tableofcontents
\end{frame}

\subsection{General problem layout}

\begin{frame}
	\frametitle{What is automatic differentiation?}

	Given a function $f: \R^{n} \longrightarrow \R$ that is
	\textit{twice differentiable}, we want to
	\textit{efficiently}
	and with good \textit{numerical stability} compute
	\vspace{5mm}

	\begin{itemize}
		\item $f(x)$, the value of our function at a point $x$ in $\R^{n}$,
		\item $\nabla f(x)$, the gradient of our function at $x$, and
		\item $\nabla^2 f(x)$, the Hessian of our function at $x$.
	\end{itemize}

	\vspace{5mm}
	Where $\nabla$ is the \textit{Differential Operator}, also called
	the \textit{nabla} Operator or sometimes just \textit{Del}.
\end{frame}

% Make this frame aligned to the top
\begin{frame}[t]
	\frametitle{Quick reminder of the symbols}

	\only<1->{
		\vspace{5mm}
		Since $f: \R^{n} \longrightarrow \R$, we know that
		$f(x) \in \R$, now we define the Gradient and Hessian
		for $x \in O \subset \R^n$ where $O$ is an
		open subset in $\R^n$ as follows:
	}

	\only<2>{
		\vspace{5mm}
		\[
			\nabla f(x) := \begin{pmatrix}
				\frac{ \partial f(x) }{ \partial x_1 } \\
				\frac{ \partial f(x) }{ \partial x_2 } \\
				\vdots                                 \\
				\frac{ \partial f(x) }{ \partial x_n }
			\end{pmatrix}
		\]

		\vspace{5mm}

		The Gradient is the n-dimensional
		\textit{vector} of partial derivatives of $f$ at $x$.
		It describes how $f$ changes with respect to each of the
		$n$ variables $x_1, \ldots, x_n$.
	}

	\only<3->{
		\[
			\nabla \left(
			\nabla f(x)
			\right) = \nabla^2 f(x) := \begin{pmatrix}
				\frac{ \partial ^2 f(x) }{ \partial x_1 ^2 }           &
				\frac{ \partial ^2 f(x) }{ \partial x_1 \partial x_2 } &
				\cdots                                                 &
				\frac{ \partial ^2 f(x) }{ \partial x_1 \partial x_n }   \\
				\frac{ \partial ^2 f(x) }{ \partial x_2 \partial x_1 } &
				\frac{ \partial ^2 f(x) }{ \partial x_2 ^2 }           &
				\cdots                                                 &
				\frac{ \partial ^2 f(x) }{ \partial x_2 \partial x_n }   \\
				\vdots                                                 &
				\vdots                                                 &
				\ddots                                                 &
				\vdots                                                   \\
				\frac{ \partial ^2 f(x) }{ \partial x_n \partial x_1 } &
				\frac{ \partial ^2 f(x) }{ \partial x_n \partial x_2 } &
				\cdots                                                 &
				\frac{ \partial ^2 f(x) }{ \partial x_n ^2 }             \\
			\end{pmatrix}
		\]
	}

	\only<3>{
		The Hessian is the real matrix of all \textit{second order}
		partial derivatives of $f$ at $x$. It describes how the gradient
		changes with respect to each of the $n$ variables $x_1, \ldots, x_n$.
	}

	\only<4>{
		Each row of the Hessian can be regarded as the gradient w.r.t. each
		component of the gradient vector of $f$. Specifically, it is the
		\textit{Jacobian} matrix of the gradient.
	}

\end{frame}

\subsection{Characterizing Sequence}
\begin{frame}[t]
	\frametitle{ Characterizing Sequence }

	\vspace{5mm}

	\only<1>{
		We will only discuss functions that can be written as compositions of:
		\vspace{5mm}

		\begin{itemize}
			\item constant functions \mathcal{C},
			\item unary functions \mathcal{U} and
			\item binary functions \mathcal{B}.
		\end{itemize}

		\vspace{5mm}
		Where \mathcal{C, U} and \mathcal{B} are the corresponding
		function spaces.

		\vspace{5mm}
		The characterizing sequence computes $f(x)$ in $m$-steps, with each
		step depending \textit{only} on previously computed steps.
	}

	\only<2>{
		We decompose the computation of $f(x)$ as follows:

		% Char sequence for f(x)
		\begin{itemize}
			\item $f_i = x_i$ for $i \in \{
				      1, \ldots, n
				      \} $
			\item $f_{i + n} = \begin{cases}
					      \omega_i                    & \text{ if } \omega_i \in \mathcal{C} \\
					      \omega_i (f_{k_i})          & \text{ if } \omega_i \in \mathcal{U} \\
					      \omega_i (f_{k_i}, f_{l_i}) & \text{ if } \omega_i \in \mathcal{B}
				      \end{cases}$
			      \qquad for $i \in \{
				      1, \ldots, m
				      \}$
			\item $f_{m + n} = f(x)$
			\item $k_i, l_i < i + n$  \text{ and }
			\item $\{
				      n+1, \ldots, n+m - 1
				      \} \subset \bigcup_{i=1}^{m} \{
				      k_i, l_i
				      \}$
		\end{itemize}
		The last condition ensures that each of the $m$-steps in the computation
		is actually used to compute $f(x)$.
		\[
			\Rightarrow S := \left(
			(\omega_i, k_i, l_i)
			\right)_{i \in \{
					1, \ldots, m
					\} }
			\text{is char. seq. for f}
		\]
		that is defined in the following.
	}

	\only<3-4>{
		More specifically, we set $I := \{
			1, \ldots, m
			\} $ and $J := \{
			1, \ldots, n + m - 1
			\} $ as two sets of indices.

		\begin{align*}
			\Rightarrow S & := \left(
			(\omega_i, k_i, l_i)
			\right)_{i \in \{
					1, \ldots, m
			\} }                       \\
			              & \in \left(
			\left(
				\mathcal{C} \times \{
				0
				\}^2
				\right) \cup
			\left(
				\mathcal{U} \times J \times \{
				0
				\} \cup
				\left(
					\mathcal{B} \times J^2
					\right)
				\right) \right) ^m
		\end{align*}

		The above is equivalent to the following three cases:
	}

	\only<4>{
		\vspace{5mm}

		\begin{itemize}
			\item if $\omega_i \in \mathcal{C} \Rightarrow
				      k_i = l_i = 0$
			\item if $\omega_i \in \mathcal{U} \Rightarrow
				      k_i \in J, l_i = 0$
			\item if $\omega_i \in \mathcal{B} \Rightarrow
				      k_i, l_i \in J$
		\end{itemize}
	}
\end{frame}

\begin{frame}
	\frametitle{The Computational Graph}
	An example computational graph constructed by a characterizing sequence
	for $n=4$ and $m=4$.

	\begin{center}
		\begin{tikzpicture}
			% Add the states
			\node[state]			 (a) {$x_1$};
			\node[state, right=of a] (b) {$x_2$};
			\node[state, right=of b] (c) {$x_3$};
			\node[state, right=of c] (d) {$x_4$};
			\node[state, below=of a] (e) {$f_{1 + n}$};
			\node[state, below=of b] (f) {$f_{2 + n}$};
			\node[state, below=of c] (g) {$f_{3 + n}$};
			\node[state, below=of f] (h) {$f_{m + n} = f(x)$};

			% Connect the states with arrows
			\draw[every loop]
			(a) edge[bend right, auto=right] node {$\omega_1 (k_1)$} (e)
			(b) edge[bend right, auto=right] node {$\omega_2 (k_2, l_2)$} (f)
			(c) edge[bend right, auto=left] node {$\omega_2 (k_2, l_2)$} (f)
			(c) edge[bend left, auto=left] node {$\omega_3 (k_3, l_3)$} (g)
			(f) edge[bend right, auto=right] node {$\omega_3 (k_3, l_3)$} (g)
			(e) edge[bend right, auto=right] node {$\omega_4 (k_4, l_4)$} (h)
			(g) edge[bend left, auto=left] node {$\omega_4 (k_4, l_4)$} (h)

		\end{tikzpicture}
	\end{center}
\end{frame}

\section{Operation of Auto-Diff}
\subsection{Forward mode}
\begin{frame}[t]
	\frametitle{Computing the Gradient $\nabla f(x)$}

	\only<1>{
		The idea is to compute the gradient and hessian of $f$ stepwise using
		the characterizing sequence $S$.
	}

	\only<2->{
		\begin{itemize}
			\item $g_j = e_j$ with $(e_j)_k = 1_{k=j}$
			      and $e_j \in \R^n$ for $j \in \{
				      1, \ldots, n
				      \}$
			\item $g_{i+n} = \begin{cases}
					      0
					       & \text{ if } \omega_i \in \mathcal{C} \\
					      \omega_i ' (f_{k_i}) g_{k_i}
					       & \text{ if } \omega_i \in \mathcal{U} \\
					      \partial_a \omega_i (f_{k_i}, f_{l_i}) g_{k_i}
					      + \partial_b \omega_i (f_{k_i}, f_{l_i}) g_{l_i}
					       & \text{ if } \omega_i \in \mathcal{B}
				      \end{cases}$ ,
			      for $i \in \{
				      1, \ldots, m
				      \}$
		\end{itemize}
		Note the following:
		\vspace{5mm}
	}

	\only<3>{
		The $g_i$ are $n$-dimensional vectors $\iff g_i \in \R^n$
	}

	\only<4>{
		For $\omega_i \in \mathcal{U}$ we have:
		\begin{gather*}
			\omega_i ' (f_{k_i}) \in \R \text{ since }
			\omega_i : \R \to \R \\
			\Rightarrow \omega_i ' (f_{k_i}) g_{k_i} \in \R^n
		\end{gather*}
	}

	\only<5>{
		For $\omega_i \in \mathcal{B}$ we have:
		\begin{align*}
			\nabla \omega_i (f_{k_i}, f_{l_i})
			 & =
			\partial_a \omega_i (f_{k_i}, f_{l_i}) \nabla f_{k_i} +
			\partial_b \omega_i (f_{k_i}, f_{l_i}) \nabla f_{l_i} \\
			 & =
			\partial_a \omega_i (f_{k_i}, f_{l_i}) g_{k_i} +
			\partial_b \omega_i (f_{k_i}, f_{l_i}) g_{l_i}
		\end{align*}

		With $\partial_a \omega_i (f_{k_i}, f_{l_i})$ being the partial derivative
		in the \textit{first} argument, that is w.r.t. $f_{k_i}$.
	}

	\only<6>{
		Thus, we have for
		$i \in \{
			1, \ldots, n + m
			\} $:
		\[
			g_i = \nabla f_i = \begin{pmatrix}
				\frac{ \partial f_i }{ \partial x_1 } \\
				\frac{ \partial f_i }{ \partial x_2 } \\
				\vdots                                \\
				\frac{ \partial f_i }{ \partial x_n } \\
			\end{pmatrix}
		\]
	}

	\only<7>{
		So for all $i \in \{
			1, \ldots, n + m
			\} $, $g_i$ is really the gradient vector of $f_i$ w.r.t. all
		inputs $\left(
			x_1, \ldots, x_n
			\right) $.

		\vspace{5mm}
		$\Rightarrow$ Auto-Diff applies the chain rule iteratively.
	}
\end{frame}

\begin{frame}[t]
	\frametitle{Computing the Hessian $\nabla^2 f(x)$}
	% Note, what I learned today --> big mathematics !! ;))
	% for vectors a and b (columns vectors both)
	% a^T b is the dot product
	% a b^T is the outer product yielding a matrix
	% (a b^T)_ij = a_i b_j

	\only<1-3>{
		\begin{itemize}
			\item $H_j = 0 \in \R^{n \times n}$ for
			      $j \in \{
				      1, \ldots, n
				      \} $
			\item $H_{i+n} = \begin{cases}
					      0
					       & \text{ if } \omega_i \in \mathcal{C} \\
					      \omega_i ' (f_{k_i}) H_{k_i} + \omega_i '' (f_{k_i})
					      g_{k_i}
					      g_{k_i} ^T
					       & \text{ if } \omega_i \in \mathcal{U} \\
					      \partial_a \omega_i (f_{k_i}, f_{l_i}) H_{k_i} +
					      \partial_b \omega_i (f_{k_i}, f_{l_i}) H_{l_i}
					       &                                      \\
					      + (g_{k_i}, g_{l_i}) \nabla ^2 \omega_i (f_{k_i}, f_{l_i})
					      (g_{k_i}, g_{l_i})^T
					       & \text{ if } \omega_i \in \mathcal{B} \\
				      \end{cases}$
		\end{itemize}
	}

	\only<1-4>{
		\vspace{5mm}
	}

	\only<1>{
		Note that $g_{k_i} g_{k_i}^T$ is a  $n \times n$ matrix
		(outer product)
		, while
		$g_{k_i} ^T g_{k_i}$ is only a single number (dot product).
	}

	\only<2>{
		Now assuming $a: \R^n \to \R$ and $b: \R^n \to \R^n$, we can see:
		\begin{align*}
			 & \nabla \left(
			a \cdot (b_1, \ldots, b_n)^T
			\right)  = \left(
			\frac{ \partial (a \cdot b_j) }{ \partial x_i }
			\right)_{i,j \in \{
				1, \ldots, n
			\} }             \\
			 & = \left(
			\frac{ \partial a }{ \partial x_i } b_j +
			a \frac{ \partial b_j }{ \partial x_i }
			\right)_{i,j}
			= \nabla a b^T + a \cdot \nabla b
		\end{align*}
	}

	\only<3>{
		Then we can easily see that:
		\begin{align*}
			\nabla \left(
			\omega_i ' (f_{k_i}) g_{k_i}
			\right)
			 & =
			\nabla (\omega_i ' (f_{k_i})) g_{k_i}^T + \omega_i ' (f_{k_i})
			\left(
			\nabla g_{k_i}
			\right)
			\\
			 & = \omega_i ' (f_{k_i}) H_{k_i} + \omega_i '' (f_{k_i})
			g_{k_i}
			g_{k_i} ^T
		\end{align*}

		With $\nabla$ being the \textit{total Differential} Operator.
	}

	\only<4>{
		Not so easy to see is the case $\omega_i \in \mathcal{B}$:
	}

	\begin{align*}
		\only<4>{
		 & \nabla \left(
			\partial_a \omega_i (f_{k_i}, f_{l_i}) g_{k_i}
			+ \partial_b \omega_i (f_{k_i}, f_{l_i}) g_{l_i}
		\right)                                                       \\
		 & = \; \nabla \left(
			\partial_a \omega_i (f_{k_i}, f_{l_i}) g_{k_i}^T
			\right)
		+ \partial_a \omega_i (f_{k_i}, f_{l_i}) \cdot \nabla g_{k_i} \\
		 & + \nabla
			\left(
			\partial_b \omega_i (f_{k_i}, f_{l_i}) g_{l_i}^T
			\right)
		+ \partial_b \omega_i (f_{k_i}, f_{l_i}) \cdot \nabla g_{l_i} \\
		\\
		}
		\only<4-5>{
		 & =  \;
			\partial_a \omega_i (f_{k_i}, f_{l_i}) \cdot H_{k_i}
		+ \partial_b \omega_i (f_{k_i}, f_{l_i}) \cdot H_{l_i}        \\
		 & + \left(
			\partial_a ^2 \omega_i ( f_{k_i}, f_{l_i} ) g_{k_i}
			+ \partial_b \partial_a \omega_i ( f_{k_i}, f_{l_i} ) g_{l_i}
		\right) g_{k_i}^T                                             \\
		 & + \left(
			\partial_b ^2 \omega_i ( f_{k_i}, f_{l_i} ) g_{l_i}
			+ \partial_a \partial_b \omega_i ( f_{k_i}, f_{l_i} ) g_{k_i}
		\right) g_{l_i}^T                                             \\
		\\
		}
		\only<5-6>{
		 & =
			\partial_a \omega_i (f_{k_i}, f_{l_i}) \cdot H_{k_i}
		+ \partial_b \omega_i (f_{k_i}, f_{l_i}) \cdot H_{l_i}        \\
		 & + \left(
			g_{k_i},
			g_{l_i}
			\right)
			\begin{pmatrix}
				\partial_a ^2 \omega_i ( f_{k_i}, f_{l_i} )         &
				\partial_a \partial_b \omega_i ( f_{k_i}, f_{l_i} )   \\
				\partial_b \partial_a \omega_i ( f_{k_i}, f_{l_i} ) &
				\partial_b ^2 \omega_i ( f_{k_i}, f_{l_i} )           \\
			\end{pmatrix}
		\begin{pmatrix}
				g_{k_i} ^T \\
				g_{l_i} ^T
			\end{pmatrix}                                                \\
		\\
		}
		\only<6>{
		 & =
			\partial_a \omega_i (f_{k_i}, f_{l_i}) \cdot H_{k_i}
		+ \partial_b \omega_i (f_{k_i}, f_{l_i}) \cdot H_{l_i}        \\
		 & +
			\underbrace{
				\left(
				g_{k_i},
				g_{l_i}
				\right)
			}_{n \times 2}
			\underbrace{
				\nabla ^2 \omega_i ( f_{k_i}, f_{l_i} )
			}_{2 \times 2}
			\underbrace{
				\left(
				g_{k_i},
				g_{l_i}
				\right) ^T
			}_{2 \times n}
		}
	\end{align*}
\end{frame}

\begin{frame}
	\frametitle{Forward mode}

	Compute \textit{sequentially}:
	\[
		f_i, \; g_i, \; H_i, \text{ for } i \in \{
		1, \ldots, n+m
		\}
	\]
	and then we obtain:
	\vspace{5mm}
	\begin{itemize}
		\item $f_{n+m} = f(x)$, the Function value,
		\item $g_{n+m} = \nabla f(x)$, the Gradient and
		\item $H_{n+m} = \nabla ^2 f(x)$, the Hessian matrix.
	\end{itemize}
	\vspace{5mm}
	This mode is called \textit{forward mode}.
\end{frame}

\subsection{Reverse mode}
\begin{frame}
	\frametitle{Reverse mode - layout}

	We define the following functions for $i \in \{
		1, \ldots, m-1
		\} $
	\begin{align*}
		F_i : \R^{n+i - 1} \to \R^{n+i},
		F_i (y_1, \ldots, y_{n+i-1}) =
		\begin{pmatrix}
			y_1       \\
			\vdots    \\
			y_{n+i-1} \\
			f_{i+n}
		\end{pmatrix}
	\end{align*}
	And for $i=m$ we set:
	\[
		F_m : \R^{n+i - 1} \to \R,
		F_m \left(
		y_1, \ldots, y_{n+m-1}
		\right) = f_{m+n}
	\]
	Technically we can only define the $F_i$ on open subsets
	of $\R^{n+i-1}$ on which the $\omega_i$ are defined,
	but here I left this out since it is obvious to see.
\end{frame}

\begin{frame}
	\frametitle{Reverse mode - layout}
	\only<1>{
	We set the intermediate state as $G_0 := x$ and:
	\[
		G_i ^{T} := \left(
		f_1, \ldots, f_{n+i}
		\right)^T
		=
		F_i \circ
		F_{i-1} \circ
		\ldots \circ
		F_1 (x)
	\]

	for
	$i \in \{
		1, \ldots, m-1
		\} $.
	Thus we have the identity:
	\[
		f(x) = f_{m+n} =
		F_m \circ
		F_{m-1} \circ
		\cdots \circ
		F_1 (x)
	\]
	Differentiating this identity w.r.t. $x$ yields:
	}
	\only<1-3>{
		\begin{align}
			\nabla f(x) ^T =
			DF_m (G_{m-1})
			DF_{m-1} (G_{m-2})
			\cdots
			DF_1 (x)
		\end{align}
	}
	\only<1>{
		Where $DF$ denotes the Jacobian Matrix of $F$.
	}

	\only<2>{
		\vspace{5mm}
		Evaluating equation (1) from \textit{right to left}
		corresponds to the forward mode of Auto-Diff.
	}

	\only<3>{
		\vspace{5mm}
		In reverse mode, we want to evaluate (1) from
		\textit{left to right}.

		\vspace{5mm}
		This obviously yields the \textit{same} gradient $\nabla f(x)$.
	}

\end{frame}

\begin{frame}
	\frametitle{Reverse mode - in detail}

	\only<1>{
		In detail we find:
		\begin{align*}
			DF_1 ( \; \overbrace{G_0}^{=x} \; ) =
			\left(
			\begin{array}
					\\
					\\
					I_n                                       \\
					\\
					\hline                                    \\
					\vspace{2mm}
					\frac{ \partial f_{1+n} }{ \partial x_1 }
					\cdots
					\frac{ \partial f_{1+n} }{ \partial x_n } \\
				\end{array}
			\right)
			\in \R^{(n+1) \times n}
		\end{align*}

		With the last row being the gradient $\nabla f_{1+n} ^T$ of $f_{1+n}$.
	}

	\only<2>{
		This also works for
		$i \in \{
			1, \ldots, m-1
			\}$
		and generalizes to:
	}

	\only<2-4>{
		\begin{align*}
			DF_i (G_{i-1}) =
			\left(
			\begin{array}
					\\
					\\
					I_{n + i - 1}                                       \\
					\\
					\hline                                              \\
					\vspace{2mm}
					\frac{ \partial f_{i+n} }{ \partial f_{1} }
					\cdots
					\frac{ \partial f_{i+n} }{ \partial f_{n + i - 1} } \\
				\end{array}
			\right)
			\in \R^{(n+i) \times (n + i - 1)}
		\end{align*}
	}

	\only<2>{
		But now the last row is the gradient of $f_{i+n}$
		\textbf{w.r.t.}
		$\left(
			f_1, \ldots, f_{n + i - 1}
			\right) $ and not $x$ !
	}

	\only<3>{
		We now recognize that the $f_{i+n}$ can only
		depend on at most 2 elements in
		$\left(
			f_1, \ldots, f_{n+i-1}
			\right) $.

		\vspace{5mm}
		Thus the last row can only contain 2 non-zero elements, namely
		at indices $k_i$ and  $l_i$.
	}

	\only<4-5>{
		So we can write:
		\begin{align*}
			DF_i (G_{i-1}) =
			\left(
			\begin{array}
					\\
					\\
					I_{n + i - 1} \\
					\\
					\hline        \\
					% \vspace{2mm}
					\kappa_i \hspace{5mm}
					\lambda_i
				\end{array}
			\right)
		\end{align*}
	}

	\only<5>{
		With $\kappa_i$ and $\lambda_i$ being the entries of the last row
		at indices $k_i$ and $l_i$ respectively, specifically:
	}

	\only<5-6>{
		\[
			\left(
			\frac{ \partial f_{i+n} }{ \partial f_1 },
			\ldots,
			\frac{ \partial f_{i+n} }{ \partial f_{n+i-1} }
			\right)
			=
			\left(
			\ldots,
			\kappa_i,
			\ldots,
			\lambda_i,
			\ldots
			\right)
		\]
	}

	\only<5>{
		The dots represent the zero-entries here.
	}

	\only<6>{
		\vspace{5mm}
		Notice that for $\omega_i \in \mathcal{U} \Rightarrow l_i = 0$
		and in that case we only have one non-zero entry.

		\vspace{5mm}
		And if $\omega_i \in \mathcal{C}$ then the last row becomes
		entirely zero.
	}
\end{frame}

\begin{frame}
	\frametitle{Reverse mode - in detail}
	We verify quickly:
	\[
		\nabla f(x) ^T =
		\underbrace{
			DF_m (G_{m-1})
		}_{(1 \times n + m - 1)}
		\underbrace{
			DF_{m-1} (G_{m-2})
		}_{(n+m-1 \times n+m-2)}
		\cdots
		\underbrace{
			DF_1 (x)
		}_{(n+1 \times n)}
	\]

	\vspace{5mm}
	And we can see
	that this equation really results in a $(1 \times n)$ matrix, i.e.
	$\nabla f(x) ^T$.
\end{frame}

\begin{frame}[t]
	\frametitle{Reverse mode - in detail}

	\only<1>{
		The reverse mode is carried out in $m$-steps (like the forward mode).
		We compute recursively:
	}

	\only<1-3>{
		\vspace{5mm}
		\[
			v^{(i)} := \begin{cases}
				DF^{(m)} (G_{m-1})
				 & \text{ if } i = m              \\
				 &                                \\
				v^{(i+1)} DF_i (G_{i-1})
				 & \text{ if } i = m-1, \ldots, 1
			\end{cases}
		\]
		\vspace{5mm}
	}

	\only<2>{
		Then $v^{(1)}$ will be $\nabla f(x) ^T$ according to the previous equation.
	}

	\only<3>{
		And we compute in the \textit{reverse} direction:
		\[
			v^{(m)} \to
			v^{(m-1)} \to
			\ldots \to
			v^{(2)} \to
			v^{(1)} =
			\nabla f(x) ^{T}
		\]
	}

	\only<4-5>{
		In detail we start with
		$v^{(m)} = \left(
			0, \ldots, \kappa_i, \ldots, \lambda_i, \ldots, 0
			\right) $
		:
		\\
		\vspace{5mm}
		And we inspect:
	}

	\begin{align*}
		\only<5-6>{
			v^{(m-1)}
		 & = v^{(m)} DF_{m-1} ( G_{m-2} )                       \\
		 & =
			\left(
			\; \kappa_i \qquad \lambda_i \;
			\right)
		\begin{pmatrix}
				1      & 0            & \ldots & \ldots        & 0      \\
				0      & 1            & \ddots & \ddots        & 0      \\
				\vdots & \vdots       & \ddots & \ddots        & \vdots \\
				0      & 0            & \ldots & \ldots        & 1      \\
				       & \kappa_{i-1} &        & \lambda_{i-1} &        \\
			\end{pmatrix} \\
		}
		\only<6-7>{
		\\
		 & =
			\left(
			v^{(m)}_1,
			v^{(m)}_2,
			\ldots
			v^{(m)}_{n+m-2},
			v^{(m)}_{n+m-1}
		\right)                                                 \\
		 & +
			\left(
			0, \ldots,
			\kappa_{m-1} v^{(m)}_{n+m},
			0, \ldots, 0,
			\kappa_{m-1} v^{(m)}_{n+m},
			\ldots, 0
			\right)
		}
	\end{align*}

	\only<7>{
		\vspace{5mm}
		To compute $v^{(i-1)}$ from $v^{(i)}$,
		we add $\kappa_{i-1} v^{(i)}_{n+i}$ to the $k_i$-th
		component and $\lambda_{i-1} v^{(i)}_{n+i}$ to the $l_i$-th
		component (if $k_i \neq 0$ and / or  $l_i \neq 0$).

		\vspace{5mm}
		And of course delete the last component of $v^{(i)}$.
	}

	\only<8->{
		Now remember that $\kappa_i$ and $\lambda_i$ are always entries
		of the gradients in $g_i$ (the gradient vector).
	}

	\only<9->{
		\vspace{5mm}
		Compute $v^{(i)}$ efficiently by decomposing $g_i$ and thus applying
		smart multiplication rules.
	}

\end{frame}

\begin{frame}
	World
\end{frame}

\section{Applications}
\subsection{Use in optimization Algorithms}

\begin{frame}
	\frametitle{Applications - Optimization Problems}

	\begin{itemize}
		\item Efficiently compute gradients \textit{and Hessians}
		      \vspace{5mm}
		\item No need to \textit{symbolically} calculate derivatives,
		      especially for complex functions
		      \vspace{5mm}
		\item Auto-Diff achieves high numerical accuracy, better than
		      numerical methods like finite differences
	\end{itemize}
\end{frame}

\subsection{Use in neural networks}

\begin{frame}[t]
	\frametitle{Applications - Neural Networks}
	General layout: Learn parameterized mapping
	$f_\theta : \mathcal{X} \to \mathcal{Y}$ from dataset
	$(x_i, y_i)_{i \in \{
				1, \ldots, n
				\} }$
	where:
	$(x_i, y_i) \in \mathcal{X} \times \mathcal{Y}$.
	\vspace{5mm}

	\begin{enumerate}
		\only<1-3>{
		\item Initialize neural network with random parameters $\theta$
		      }
		      \only<2-3>{
		      \vspace{5mm}
		\item For samples from a dataset $(x_i, y_i)$,
		      calculate $f_\theta(x_i) = \hat{y}_i$ and the \textit{gradient}
		      of specified \textit{loss function} $L$ w.r.t. $\theta$:
		      \[
			      \nabla_\theta L(\hat{y}_i, y_i)
		      \]
		      }
		      \only<3>{
		\item Update the parameters $\theta$ using the computed
		      \textit{gradient}
		      }
	\end{enumerate}

	\only<4>{
		$\rightarrow$ Auto-Diff computes mentioned gradient \textit{at the same time}
		when computing $\hat{y}_i$ (the network outputs)
	}
\end{frame}

\section{Literature}
\begin{frame}
	\frametitle{Literature used}
	\begin{itemize}
		\item "Automatic Differentiation: A Structure-Exploiting Forward
		      Mode with Almost Optimal Complexity for Kantorovic Trees"
		      - Michael Ulbrich and Stefan Ulbrich January 1996
	\end{itemize}
\end{frame}

\begin{frame}
	\begin{center}
		\Large{Thank you for your attention!}
	\end{center}
\end{frame}

\end{document}
