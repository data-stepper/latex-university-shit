\documentclass[11pt,a4paper]{article}

\usepackage[margin=1in]{geometry}
\usepackage{amsmath,amsthm,amssymb, graphicx, multicol, array}

\usepackage{tikz}
\usetikzlibrary{automata, positioning}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{enumerate}

\usepackage{mathrsfs}

\usepackage{color}
\usepackage{epsfig}

%--------------------------------------------------------------------------
% Maths Macros
%--------------------------------------------------------------------------
\def\Z{{\mathbb Z}}
\def\N{{\mathbb N}}
\def\Q{{\mathbb Q}}
\def\R{{\mathbb R}}
\def\C{{\mathbb C}}
\def\S{{\mathbb S}}
\def\K{{\mathbb K}}
\def\T{{\mathbb T}}

\def\cA{{\mathcal A}}
\def\cF{{\mathcal F}}
\def\cG{{\mathcal G}}
\def\cM{{\mathcal M}}
\def\cN{{\mathcal N}}
\def\cP{{\mathcal P}}
\def\cS{{\mathcal S}}

%
\def\sC{\mathscr{C}}

\newtheorem{algorithm}{Algorithm}[section]
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}{Proposition}[section]
\newtheorem{lemma}{Lemma}[section]

\theoremstyle{definition} % Makes my definitions non italic
\newtheorem{definition}{Definition}[section]

\def\qed{\hfill{\rule{1.5ex}{1.5ex}}}
\def\eop{\hfill{$\Box$}}

\begin{document}
\title{ \textbf{Handout - Automatic Differentiation} }
\author{Bent Mueller}
\date{7.6.2022}
\maketitle

\section{Overview}

Given a \textit{twice differentiable} function $f: \R^n \to \R$, the
Auto-Diff Algorithm \textit{efficiently} computes gradient and Hessian
while achieving good \textit{numerical stability}:

\[
	\nabla f(x) := \begin{pmatrix}
		\frac{\partial f(x)}{\partial x_1} \\
		\frac{\partial f(x)}{\partial x_2} \\
		\vdots                             \\
		\frac{\partial f(x)}{\partial x_n}
	\end{pmatrix} ;
	\qquad
	\nabla ^2 f(x) :=
	\begin{pmatrix}
		\frac{\partial^2 f(x)}{\partial x_1^2}            &
		\frac{\partial^2 f(x)}{\partial x_1 \partial x_2} &
		\vdots                                            &
		\frac{\partial^2 f(x)}{\partial x_1 \partial x_n}   \\
		\frac{\partial^2 f(x)}{\partial x_2 \partial x_1} &
		\frac{\partial^2 f(x)}{\partial x_2 \partial x_2} &
		\vdots                                            &
		\frac{\partial^2 f(x)}{\partial x_2 \partial x_n}   \\
		\vdots                                            &
		\vdots                                            &
		\vdots                                            &
		\vdots                                              \\
		\frac{\partial^2 f(x)}{\partial x_n \partial x_1} &
		\frac{\partial^2 f(x)}{\partial x_n \partial x_2} &
		\vdots                                            &
		\frac{\partial^2 f(x)}{\partial x_n \partial x_n}
	\end{pmatrix}
\]

The Hessian is the \textit{Jacobian} of the gradient.

\subsection{Characterizing Sequence}

We only consider functions $f$ which can be decomposed into:

\begin{itemize}
	\item Constant functions $f(x) = c \in \R$, we say $f \in \mathcal{C}$
	\item Unary functions $f(x) \in \R$, we say $f \in \mathcal{U}$
	\item Binary functions $f(x_1, x_2) \in \R$, we say $f \in \mathcal{B}$
\end{itemize}

Note that a constant function takes no argument, a unary function takes one
and a binary function takes two arguments.
Thus we can decompose $f$ as follows:

% Char sequence for f(x)
\begin{enumerate}[(1)]
	\item $f_i = x_i$ for $i \in \{
		      1, \ldots, n
		      \} $
	\item $f_{i + n} = \begin{cases}
			      \omega_i                    & \text{ if } \omega_i \in \mathcal{C} \\
			      \omega_i (f_{k_i})          & \text{ if } \omega_i \in \mathcal{U} \\
			      \omega_i (f_{k_i}, f_{l_i}) & \text{ if } \omega_i \in \mathcal{B}
		      \end{cases}$
	      \qquad for $i \in \{
		      1, \ldots, m
		      \}$
	\item $f_{m + n} = f(x)$
	\item $k_i, l_i < i + n$  \text{ and }
	\item $\{
		      n+1, \ldots, n+m - 1
		      \} \subset \bigcup_{i=1}^{m} \{
		      k_i, l_i
		      \}$
\end{enumerate}

We define two sets of indices, $I := \{
	1, \ldots, m
	\} $ and $J := \{
	1, \ldots, n+m-1
	\} $.
Then we say that a sequence $S$ of tuples is a \textit{characterizing sequence}
for $f$ if and only if:

\[
	S = \left(
	(\omega_i, k_i, l_i)
	\right)_{i \in I}
	\in \left(
	\left(
		\mathcal{C} \times \{
		0
		\}^2
		\right) \cup
	\left(
		\mathcal{U} \times J \times \{
		0
		\} \right) \cup
	\left(
		\mathcal{B} \times J^2
		\right)
	\right) ^m
\]

Such that $S$ fulfills conditions 1-5 from the above.
$\Rightarrow$ $S$ computes $f(x)$ in $m$ steps.
Now we can use $S$ to also compute gradient and Hessian of $f$.

\subsection{Computing Gradient $\nabla f(x)$}

We define the sequence for the gradient as follows,
$g_j = e_j$ (unit vectors in $\R^n$) for $i \in \{
	1, \ldots, n
	\} $ and

\[
	g_{i+n} = \begin{cases}
		0
		 & \text{ if } \omega_i \in \mathcal{C} \\
		\omega_i ' (f_{k_i}) g_{k_i}
		 & \text{ if } \omega_i \in \mathcal{U} \\
		\partial_a \omega_i (f_{k_i}, f_{l_i}) g_{k_i}
		+ \partial_b \omega_i (f_{k_i}, f_{l_i}) g_{l_i}
		 & \text{ if } \omega_i \in \mathcal{B}
	\end{cases} \quad \text{ for }
	i \in \{
	1, \ldots, m
	\}.
\]

At each step, $g_{i+n}$ is exactly the gradient of $f_{i+n}$
w.r.t. $x$ ($\Leftrightarrow g_{i+n} = \nabla f_{i+n}$).
Computing this sequence for $i$ in \textit{ascending} order gives us
the so-called \textit{forward mode} and yields the gradient of $f$
at the last step: $\nabla f(x) = g_{m+n} (x)$. Also note here that
$\partial_a \omega_i (f_{k_i}, f_{l_i})$ denotes the partial derivative
\textit{in the first argument} of $\omega_i$, that is w.r.t. $f_{k_i}$.

\subsubsection{Example of the Forward Mode}
Suppose we want to compute the gradient of
$f(x, y) = \sqrt{y} \exp(x^2  - y)$
then we find a characterizing sequence for $f$ as follows:

\[
	f_1=x,
	f_2=y,
	f_3=\sqrt{f_2},
	f_4=f_1^2,
	f_5=\exp(f_4 - f_2),
	\Rightarrow f_6=f_3 f_5 = f(x, y)
\]

\[
	\Rightarrow S = \left(
	(\sqrt{\cdot}, 2, 0),
	((\cdot)^2, 1, 0),
	(\exp(\cdot - \cdot), 4, 2),
	((\cdot)(\cdot), 3, 5)
	\right)
\]

We can see, $S$ computes $f(x,y)$ in four steps ($m=4, n=2$).
Now we differentiate the $f_{i+n}$ to obtain the $g_{i+n}$:

\[
	g_1 = \begin{pmatrix} 1 \\ 0 \end{pmatrix},
	g_2 = \begin{pmatrix} 0 \\ 1 \end{pmatrix},
	g_3 =
	\frac{ g_2 }{ 2 \sqrt{f_2} } =
	\frac{ 1 }{ 2 \sqrt{y} } \begin{pmatrix} 0 \\ 1 \end{pmatrix},
	g_4 = 2 x \begin{pmatrix} 1 \\ 0 \end{pmatrix}
\]

And now for the binary functions at last we have:

\begin{gather*}
	g_5 =
	\partial_{f_4} \exp (f_4 - f_2) g_4
	+ \partial_{f_2} \exp (f_4 - f_2) g_2
	= \begin{pmatrix}
		\exp (x^2 - y) 2x \\
		- \exp (x^2 - y)
	\end{pmatrix}
	\text{ and finally } \\ \\
	g_6 =
	\partial_{f_3} ( f_3 \cdot f_5 ) g_3 +
	\partial_{f_5} ( f_3 \cdot f_5 ) g_5
	= f_5 g_3 + f_3 g_5 \\
	=
	\exp (x^2 - y) \begin{pmatrix} 0 \\
		\frac{ 1 }{ 2 \sqrt{y} }
	\end{pmatrix}
	+ \sqrt{y}
	\begin{pmatrix}
		\exp (x^2 - y) 2x \\
		- \exp (x^2 - y)
	\end{pmatrix}
	=
	\exp (x^2 - y)
	\begin{pmatrix}
		2x \sqrt{y} \\
		\frac{ 1 }{ 2 \sqrt{y} } - \sqrt{y}
	\end{pmatrix}
	= \nabla f(x, y)
\end{gather*}

And we can easily verify that $g_6$ is the gradient of $f(x,y)$.
Note that here we left $x$ and $y$ free variables but in practice, the
algorithm would compute only $f(x,y)$ as well as its gradient
$\nabla f(x,y)$ at specific points $(x,y)$. Thus in the gradient computation,
this algorithm exploits that we already computed $f_{i+n}$ at the relevant
points $(x,y)$ saving computational resources.

\subsection{Computing Hessian $\nabla^2 f(x)$}

We define the sequence for the Hessian as $H_j = 0 \in \R^{n \times n}$
for
$j \in \{
	1, \ldots, n
	\} $ and

\[
	H_{i+n} = \begin{cases}
		0
		 & \text{ if } \omega_i \in \mathcal{C} \\
		\omega_i ' (f_{k_i}) H_{k_i} + \omega_i '' (f_{k_i})
		g_{k_i}
		g_{k_i} ^T
		 & \text{ if } \omega_i \in \mathcal{U} \\ \\
		\partial_a \omega_i (f_{k_i}, f_{l_i}) H_{k_i} +
		\partial_b \omega_i (f_{k_i}, f_{l_i}) H_{l_i}
		 &                                      \\
		+ (g_{k_i}, g_{l_i}) \nabla ^2 \omega_i (f_{k_i}, f_{l_i})
		(g_{k_i}, g_{l_i})^T
		 & \text{ if } \omega_i \in \mathcal{B} \\
	\end{cases}
	\quad \text{ for }
	i \in \{
	1, \ldots, m
	\}.
\]

Note that each $H_{i+n}$ is a $(n \times n)$ matrix and this sequence
requires that the gradients $g_i$ are already computed.

\end{document}
