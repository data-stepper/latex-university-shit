\documentclass[11pt,a4paper]{article}

\usepackage[margin=1in]{geometry}
\usepackage{amsmath,amsthm,amssymb, graphicx, multicol, array}

\usepackage{tikz}
\usetikzlibrary{automata, positioning}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{enumerate}

\usepackage{mathrsfs}

\usepackage{color}
\usepackage{epsfig}

%--------------------------------------------------------------------------
% Maths Macros
%--------------------------------------------------------------------------
\def\Z{{\mathbb Z}}
\def\N{{\mathbb N}}
\def\Q{{\mathbb Q}}
\def\R{{\mathbb R}}
\def\C{{\mathbb C}}
\def\S{{\mathbb S}}
\def\K{{\mathbb K}}
\def\T{{\mathbb T}}

\def\cA{{\mathcal A}}
\def\cF{{\mathcal F}}
\def\cG{{\mathcal G}}
\def\cM{{\mathcal M}}
\def\cN{{\mathcal N}}
\def\cP{{\mathcal P}}
\def\cS{{\mathcal S}}

%
\def\sC{\mathscr{C}}

\newtheorem{algorithm}{Algorithm}[section]
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}{Proposition}[section]
\newtheorem{lemma}{Lemma}[section]

\theoremstyle{definition} % Makes my definitions non italic
\newtheorem{definition}{Definition}[section]

\def\qed{\hfill{\rule{1.5ex}{1.5ex}}}
\def\eop{\hfill{$\Box$}}

\begin{document}
\title{ \textbf{Handout - Automatic Differentiation} }
\author{Bent Mueller}
\date{7.6.2022}
\maketitle

\section{A Note on the Notation used}

In the literature, there is no clear definition of what exactly the
$\nabla$-Operator (Nabla) is, we use it as the \textit{total derivative}
of a function with respect to all variables in the context.
Hence if $f: \R^n \to \R$ then $\nabla f: \R^n \to \R^n$ and represents
in this case the gradient vector of $f$. But if $f: \R^n \to \R^n$ then
$\nabla f: \R^n \to \R^{n \times n}$ and represents the Jacobian matrix
of $f$. When we explicitly use the Jacobian Operator $D$ it is meant to point out
that the underlying function is valued in $\R^n$.
With this Notation, $\nabla ^2 f(x)$ denotes the Hessian Matrix of $f$ and
\textbf{not} the Laplacian Operator $\Delta$ which is also sometimes
referred to as $\nabla ^2$.
\\

For two \textit{column} vectors $x, y \in \R^n$ we note that the matrix product
has the following feature:

\[
	a^{T} b := \sum_{i=1}^n a_i b_i \in \R
	\quad
	\text{ but }
	\quad
	a b^{T} :=
	\begin{pmatrix} a_1 \\ \vdots \\ a_n \end{pmatrix}
	\cdot
	\begin{pmatrix}
		b_1, \ldots, b_n
	\end{pmatrix}
	=
	\begin{pmatrix}
		a_1 b_1 & a_1 b_2 & \cdots & a_1 b_n \\
		a_2 b_1 & a_2 b_2 & \cdots & a_2 b_n \\
		\vdots  & \vdots  & \ddots & \vdots  \\
		a_n b_1 & a_n b_2 & \cdots & a_n b_n
	\end{pmatrix}
	\in \R^{n \times n}
\]

\section{Overview}

Given a \textit{twice differentiable} function $f: \R^n \to \R$, the
Auto-Diff Algorithm \textit{efficiently} computes gradient and Hessian
while achieving good \textit{numerical stability}:

\[
	\nabla f(x) := \begin{pmatrix}
		\frac{\partial f(x)}{\partial x_1} \\
		\frac{\partial f(x)}{\partial x_2} \\
		\vdots                             \\
		\frac{\partial f(x)}{\partial x_n}
	\end{pmatrix} ;
	\qquad
	\nabla ^2 f(x) :=
	\begin{pmatrix}
		\frac{\partial^2 f(x)}{\partial x_1^2}            &
		\frac{\partial^2 f(x)}{\partial x_1 \partial x_2} &
		\vdots                                            &
		\frac{\partial^2 f(x)}{\partial x_1 \partial x_n}   \\
		\frac{\partial^2 f(x)}{\partial x_2 \partial x_1} &
		\frac{\partial^2 f(x)}{\partial x_2 \partial x_2} &
		\vdots                                            &
		\frac{\partial^2 f(x)}{\partial x_2 \partial x_n}   \\
		\vdots                                            &
		\vdots                                            &
		\vdots                                            &
		\vdots                                              \\
		\frac{\partial^2 f(x)}{\partial x_n \partial x_1} &
		\frac{\partial^2 f(x)}{\partial x_n \partial x_2} &
		\vdots                                            &
		\frac{\partial^2 f(x)}{\partial x_n \partial x_n}
	\end{pmatrix}
\]

The Hessian is the \textit{Jacobian} of the gradient.

\subsection{Characterizing Sequence}

We only consider functions $f$ which can be decomposed into:

\begin{itemize}
	\item Constant functions $f(x) = c \in \R$, we say $f \in \mathcal{C}$
	\item Unary functions $f(x) \in \R$, we say $f \in \mathcal{U}$
	\item Binary functions $f(x_1, x_2) \in \R$, we say $f \in \mathcal{B}$
\end{itemize}

Note that a constant function takes no argument, a unary function takes one
and a binary function takes two arguments.
Thus we can decompose $f$ as follows:

% Char sequence for f(x)
\begin{enumerate}[(1)]
	\item $f_i = x_i$ for $i \in \{
		      1, \ldots, n
		      \} $
	\item $f_{i + n} = \begin{cases}
			      \omega_i                    & \text{ if } \omega_i \in \mathcal{C} \\
			      \omega_i (f_{k_i})          & \text{ if } \omega_i \in \mathcal{U} \\
			      \omega_i (f_{k_i}, f_{l_i}) & \text{ if } \omega_i \in \mathcal{B}
		      \end{cases}$
	      \qquad for $i \in \{
		      1, \ldots, m
		      \}$
	\item $f_{m + n} = f(x)$
	\item $k_i, l_i < i + n$  \text{ and }
	\item $\{
		      n+1, \ldots, n+m - 1
		      \} \subset \bigcup_{i=1}^{m} \{
		      k_i, l_i
		      \}$
\end{enumerate}

We define two sets of indices, $I := \{
	1, \ldots, m
	\} $ and $J := \{
	1, \ldots, n+m-1
	\} $.
Then we say that a sequence $S$ of tuples is a \textit{characterizing sequence}
for $f$ if and only if:

\[
	S = \left(
	(\omega_i, k_i, l_i)
	\right)_{i \in I}
	\in \left(
	\left(
		\mathcal{C} \times \{
		0
		\}^2
		\right) \cup
	\left(
		\mathcal{U} \times J \times \{
		0
		\} \right) \cup
	\left(
		\mathcal{B} \times J^2
		\right)
	\right) ^m
\]

Such that $S$ fulfills conditions 1-5 from the above.
$\Rightarrow$ $S$ computes $f(x)$ in $m$ steps.
Now we can use $S$ to also compute gradient and Hessian of $f$.

\subsection{Computing the Gradient $\nabla f(x)$}

We define the sequence for the gradient as follows,
$g_j = e_j$ (unit vectors in $\R^n$) for $i \in \{
	1, \ldots, n
	\} $ and

\[
	g_{i+n} = \begin{cases}
		0
		 & \text{ if } \omega_i \in \mathcal{C} \\
		\omega_i ' (f_{k_i}) g_{k_i}
		 & \text{ if } \omega_i \in \mathcal{U} \\
		\partial_a \omega_i (f_{k_i}, f_{l_i}) g_{k_i}
		+ \partial_b \omega_i (f_{k_i}, f_{l_i}) g_{l_i}
		 & \text{ if } \omega_i \in \mathcal{B}
	\end{cases} \quad \text{ for }
	i \in \{
	1, \ldots, m
	\}.
\]

At each step, $g_{i+n}$ is exactly the gradient of $f_{i+n}$
w.r.t. $x$ ($\Leftrightarrow g_{i+n} = \nabla f_{i+n}$).
Computing this sequence for $i$ in \textit{ascending} order gives us
the so-called \textit{forward mode} and yields the gradient of $f$
at the last step: $\nabla f(x) = g_{m+n} (x)$. Also note here that
$\partial_a \omega_i (f_{k_i}, f_{l_i})$ denotes the partial derivative
\textit{in the first argument} of $\omega_i$, that is w.r.t. $f_{k_i}$.

\subsubsection{Example of the Forward Mode}
Suppose we want to compute the gradient of
$f(x, y) = \sqrt{y} \exp(x^2  - y)$
then we find a characterizing sequence for $f$ as follows:

\[
	f_1=x,
	f_2=y,
	f_3=\sqrt{f_2},
	f_4=f_1^2,
	f_5=\exp(f_4 - f_2),
	\Rightarrow f_6=f_3 f_5 = f(x, y)
\]

\[
	\Rightarrow S = \left(
	(\sqrt{\cdot}, 2, 0),
	((\cdot)^2, 1, 0),
	(\exp(\cdot - \cdot), 4, 2),
	((\cdot)(\cdot), 3, 5)
	\right)
\]

We can see, $S$ computes $f(x,y)$ in four steps ($m=4, n=2$).
Now we differentiate the $f_{i+n}$ to obtain the $g_{i+n}$:

\[
	g_1 = \begin{pmatrix} 1 \\ 0 \end{pmatrix},
	g_2 = \begin{pmatrix} 0 \\ 1 \end{pmatrix},
	g_3 =
	\frac{ g_2 }{ 2 \sqrt{f_2} } =
	\frac{ 1 }{ 2 \sqrt{y} } \begin{pmatrix} 0 \\ 1 \end{pmatrix},
	g_4 = 2 x \begin{pmatrix} 1 \\ 0 \end{pmatrix}
\]

And now for the binary functions at last we have:

\begin{gather*}
	g_5 =
	\partial_{f_4} \exp (f_4 - f_2) g_4
	+ \partial_{f_2} \exp (f_4 - f_2) g_2
	= \begin{pmatrix}
		\exp (x^2 - y) 2x \\
		- \exp (x^2 - y)
	\end{pmatrix}
	\text{ and finally } \\ \\
	g_6 =
	\partial_{f_3} ( f_3 \cdot f_5 ) g_3 +
	\partial_{f_5} ( f_3 \cdot f_5 ) g_5
	= f_5 g_3 + f_3 g_5 \\
	=
	\exp (x^2 - y) \begin{pmatrix} 0 \\
		\frac{ 1 }{ 2 \sqrt{y} }
	\end{pmatrix}
	+ \sqrt{y}
	\begin{pmatrix}
		\exp (x^2 - y) 2x \\
		- \exp (x^2 - y)
	\end{pmatrix}
	=
	\exp (x^2 - y)
	\begin{pmatrix}
		2x \sqrt{y} \\
		\frac{ 1 }{ 2 \sqrt{y} } - \sqrt{y}
	\end{pmatrix}
	= \nabla f(x, y)
\end{gather*}

And we can easily verify that $g_6$ is the gradient of $f(x,y)$.
Note that here we left $x$ and $y$ free variables but in practice, the
algorithm would compute only $f(x,y)$ as well as its gradient
$\nabla f(x,y)$ at specific points $(x,y)$. Thus in the gradient computation,
this algorithm exploits that we already computed $f_{i+n}$ at the relevant
points $(x,y)$ saving computational resources.

\subsection{Computing the Hessian $\nabla^2 f(x)$}

We define the sequence for the Hessian as $H_j = 0 \in \R^{n \times n}$
for
$j \in \{
	1, \ldots, n
	\} $ and

\[
	H_{i+n} = \begin{cases}
		0
		 & \text{ if } \omega_i \in \mathcal{C} \\
		\omega_i ' (f_{k_i}) H_{k_i} + \omega_i '' (f_{k_i})
		g_{k_i}
		g_{k_i} ^T
		 & \text{ if } \omega_i \in \mathcal{U} \\ \\
		\partial_a \omega_i (f_{k_i}, f_{l_i}) H_{k_i} +
		\partial_b \omega_i (f_{k_i}, f_{l_i}) H_{l_i}
		 &                                      \\
		+ (g_{k_i}, g_{l_i}) \nabla ^2 \omega_i (f_{k_i}, f_{l_i})
		(g_{k_i}, g_{l_i})^T
		 & \text{ if } \omega_i \in \mathcal{B} \\
	\end{cases}
	\quad \text{ for }
	i \in \{
	1, \ldots, m
	\}.
\]

Note that each $H_{i+n}$ is a $(n \times n)$ matrix and this sequence
requires that the gradients $g_{i+n}$ are already computed.

\subsection{Reverse Mode - Overview}

In general, we can interpret the characterizing sequence
for $f$,
with functions $F$ and states $G$,
as follows:
\\

For $i \in \{
	1, \ldots, m-1
	\} $:
\begin{align*}
	F_i : \R^{n+i - 1} \to \R^{n+i},
	F_i (y_1, \ldots, y_{n+i-1}) =
	\begin{pmatrix}
		y_1       \\
		\vdots    \\
		y_{n+i-1} \\
		f_{i+n}
	\end{pmatrix}
\end{align*}

And for $i=m$:
\[
	F_m : \R^{n+i - 1} \to \R,
	F_m \left(
	y_1, \ldots, y_{n+m-1}
	\right) = f_{m+n}
\]

Now we notice that:
\begin{equation}
	f(x) = f_{m+n}
	=
	F_m \circ
	F_{m-1} \circ \dots \circ
	F_1 \circ
	F_0 (x)
\end{equation}

We set the intermediate state as $G_0 := x$ and
\[
	G_i ^{T} := \left(
	f_1, \ldots, f_{n+i}
	\right)^T
	=
	F_i \circ
	F_{i-1} \circ
	\ldots \circ
	F_1 (x)
\]

for
$i \in \{
	1, \ldots, m-1
	\} $.
$G_i$ represents the characterizing sequence's intermediate state
at computation step $i$.
Now if we differentiate equation (1) with respect to $x$ we get
\begin{align}
	\nabla f(x) ^T =
	DF_m (G_{m-1})
	DF_{m-1} (G_{m-2})
	\cdots
	DF_1 (x)
\end{align}

by employing the chain rule. Where $DF$ denotes the Jacobian
Matrix of $F$.

\subsubsection{Inspecting the Jacobian Matrices}

Since $F_i$ is the identity in up to the $(n+i-1)$-th
component and only actually computes something in the last component,
we can easily deduct how its Jacobian Matrix must look like.
Furthermore, the last component can at most only depend
on two previous components.

\[
	DF_i =
	\begin{pmatrix}
		1      & 0        & \cdots    & 0      \\
		0      & 1        & \cdots    & 0      \\
		\vdots & \vdots   & \ddots    & \vdots \\
		0      & 0        & \cdots    & 1      \\
		       & \kappa_i & \lambda_i &        \\
	\end{pmatrix}
	\in
	\R^{(n+i) \times (n+i-1)}
\]

Denote that the last \textit{row} is the zero vector everywhere,
except on the entries $k_i$ and  $l_i$, where its components are
$\kappa_i$ and $\lambda_i$ respectively. With  $\kappa_i$ and
$\lambda_i$ being the components of the gradient $g_i$ of $f_i$
w.r.t. $f_{i-1}$.
\\

Notice that in forward mode we evaluate expression (2) from
\textit{right to left} but in reverse mode we evaluate it from
\textit{left to right}. Also note that in reverse mode,
all the $f_i$ already need to be computed, since otherwise
we don't know the $G_i$.

\end{document}
