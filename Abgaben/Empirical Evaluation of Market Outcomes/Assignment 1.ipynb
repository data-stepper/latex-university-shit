{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f6309daae14e250",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## 1. Two-Stage Least Squares"
   ]
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "ExecuteTime": {
     "end_time": "2025-05-06T11:51:11.310248Z",
     "start_time": "2025-05-06T11:51:11.190475Z"
    }
   },
   "source": [
    "from typing import Tuple\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import numpy as np\n",
    "from scipy.stats import bootstrap\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "def least_squares(y: np.ndarray, x: np.ndarray) -> Tuple[float, float]:\n",
    "    assert y.ndim == 1\n",
    "    assert x.ndim == 1\n",
    "\n",
    "    x_ = np.stack((np.ones_like(x), x), axis=1)  # Shape: (N, 2)\n",
    "    xtx = np.dot(x_.T, x_)  # Shape: (2, 2)\n",
    "    xty = np.dot(x_.T, y)  # Shape: (2,)\n",
    "\n",
    "    beta = np.linalg.solve(xtx, xty)  # Shape: (2,)\n",
    "\n",
    "    return beta[0], beta[1]  # Intercept, slope\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DGP:\n",
    "    N: int = 1_000\n",
    "    number_of_simulations: int = 1_000\n",
    "\n",
    "    # Correlations\n",
    "    rho_xu: float = 0.5\n",
    "    rho_zx: float = 0.4\n",
    "    rho_zq: float = 0.4\n",
    "\n",
    "    true_beta_0: float = 2.0\n",
    "    true_beta_1: float = 5.0\n",
    "\n",
    "    def __call__(self) -> np.ndarray:\n",
    "        \"\"\"Runs the simulation, returns the estimates of beta_1.\"\"\"\n",
    "        estimates = np.full(shape=(self.number_of_simulations,), fill_value=np.nan)\n",
    "\n",
    "        # Generate the data\n",
    "        # NOTE: Keeping the full shape can be memory intensive for large N or number_of_simulations\n",
    "        u = np.random.standard_normal(size=(self.number_of_simulations, self.N))\n",
    "        x = self.rho_xu * u + np.random.standard_normal(size=(self.number_of_simulations, self.N))\n",
    "        q = np.random.standard_normal(size=(self.number_of_simulations, self.N))\n",
    "        z = self.rho_zx * x + self.rho_zq * q + np.random.standard_normal(size=(self.number_of_simulations, self.N))\n",
    "\n",
    "        y = self.true_beta_0 + self.true_beta_1 * x + u + 2 * q\n",
    "\n",
    "        for i in range(self.number_of_simulations):\n",
    "            # Run the two-stage least squares regression\n",
    "            # First stage: regress x on z\n",
    "            b0, b1 = least_squares(x[i], z[i])\n",
    "            x_hat = b0 + b1 * z[i]\n",
    "\n",
    "            # Second stage: regress y on x_hat\n",
    "            b0, b1 = least_squares(y[i], x_hat)\n",
    "            estimates[i] = b1\n",
    "\n",
    "        return estimates\n",
    "\n",
    "\n",
    "dgp = DGP()\n",
    "estimates = dgp()\n",
    "\n",
    "avg_beta_1 = np.mean(estimates)\n",
    "bias = avg_beta_1 - dgp.true_beta_1\n",
    "\n",
    "print(\"Mean of beta_1 estimates: {:.4f}\".format(avg_beta_1))\n",
    "print(\"Bias of beta_1 estimates: {:.4f}\".format(bias))\n",
    "\n",
    "bs = bootstrap(data=[estimates - dgp.true_beta_1], statistic=np.mean)\n",
    "low, high = bs.confidence_interval\n",
    "print(\"95% CI of Estimator Bias: [{:.4f}, {:.4f}]\".format(low, high))\n",
    "\n",
    "print(\"True beta_1: {:.4f}\".format(dgp.true_beta_1))\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean of beta_1 estimates: 7.0019\n",
      "Bias of beta_1 estimates: 2.0019\n",
      "95% CI of Estimator Bias: [1.9895, 2.0149]\n",
      "True beta_1: 5.0000\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "id": "bd0acdbf2aa777c0",
   "metadata": {},
   "source": [
    "To clarify, the Bias of an estimator is its _expected deviation from the true value_.\n",
    "Since this is an expectation, and we have samples of $\\beta_1$, we can easily estimate a confidence interval for the bias\n",
    "using the bootstrap method.\n",
    "\n",
    "For this configuration, we can say that with $\\alpha = 95 \\%$ confidence, the estimator is biased since $0$ is not in the confidence interval."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e180bc8cf432358d",
   "metadata": {},
   "source": [
    "## 2. Maximum Likelihood of Exponentially Distributed $X$\n",
    "\n",
    "$$X \\sim \\text{Exp}(\\lambda)$$\n",
    "so that the probability density function (PDF) is given by:\n",
    "$$f(x; \\lambda) = \\lambda e^{-\\lambda x}$$\n",
    "for $\\lambda, x > 0$.\n",
    "\n",
    "### (a) Likelihood and Log-Likelihood Functions\n",
    "\n",
    "For a sample of $N$ observed $x_i$'s drawn from the distribution, the likelihood function is given by:\n",
    "$$L(\\lambda) = \\prod_{i=1}^{N} f(x_i; \\lambda) = \\prod_{i=1}^{N} \\lambda e^{-\\lambda x_i} = \\lambda^N \\exp(-\\lambda \\sum_{i=1}^{N} x_i)$$\n",
    "taking logarithms, we get the log-likelihood function:\n",
    "$$\\ell(\\lambda) = \\log L(\\lambda) = N \\log \\lambda - \\lambda \\sum_{i=1}^{N} x_i$$\n",
    "where $\\sum_{i=1}^{N} x_i$ is the sum of the observed values.\n",
    "\n",
    "### (b) Maximum Likelihood Estimation of $\\lambda$\n",
    "\n",
    "The first order condition is given by:\n",
    "$$\\frac{\\partial \\ell(\\lambda)}{\\partial \\lambda} = \\frac{N}{\\lambda} - \\sum_{i=1}^{N} x_i = 0 \\implies \\frac{N}{\\lambda} = \\sum_{i=1}^{N} x_i.$$\n",
    "From which the maximum likelihood estimator (MLE) turns out to be:\n",
    "$$\\hat{\\lambda} = \\frac{N}{\\sum_{i=1}^{N} x_i}$$\n",
    "Note that the right hand side (RHS) is defined because none of the $x_i$'s are exactly $0$ (in fact, with probability $0$ or when $\\lambda \\to \\infty$).\n",
    "In practice, however, rounding errors or numerical accuracies can lead to $x_i = 0$,\n",
    "so we must carefully inspect the sample before applying the estimator.\n",
    "\n",
    "### (c) Asymptotic Variance of MLE\n",
    "\n",
    "We know that as $N \\to \\infty$, the MLE is distributed as:\n",
    "$$\\hat{\\theta} \\xrightarrow{d} N(\\theta_0, [I(\\theta_0)]^{-1})$$\n",
    "where $I(\\theta_0)$ is the Fisher information matrix evaluated at $\\theta_0$, which is\n",
    "for now assumed to be known a priori.\n",
    "Hence, the _asymptotic variance_ of the MLE, which in the multivariate case is a variance-covariance matrix, is given by\n",
    "$I(\\theta_0)^{-1}$.\n",
    "The calculation of the variance is thus a simple (matrix) inversion of the Fisher information matrix.\n",
    "\n",
    "Further, the cramer-rao lower bound (CRLB) states that the variance of any unbiased estimator $\\hat{\\theta}$\n",
    "is bounded by the inverse of the Fisher information matrix:\n",
    "$$\\text{Var}(\\hat{\\theta}) \\geq I(\\theta_0)^{-1}$$\n",
    "the right-hand side of which is the _asymptotic variance_ of the MLE.\n",
    "This can be used to place a best-case, or lower bound, on the variance of our MLE estimator.\n"
   ]
  }
 ],
 "metadata": {
  "authors": [
   {
    "name": "Bent Mueller"
   }
  ],
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
